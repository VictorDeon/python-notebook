{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8f6ebb2-bd1f-472b-9df9-3315acf55566",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Camada Gold (Delta): Criação de Fatos e Dimensões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6398c78-3c20-4556-879c-1a7d68216bcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession with the required configurations for Delta Lake\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Carga Delta\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "323ae8a3-5aa0-43ef-bbbf-ffcc462406f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define os caminhos de armazenamento no Data Lake\n",
    "silver_path = \"/mnt/lhdw/silver/vendas\"\n",
    "gold_path = \"/mnt/lhdw/gold/vendas_delta\"\n",
    "gold_fato_path = \"/mnt/lhdw/gold/vendas_delta/fato_vendas\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42e123f4-c2b6-4147-ad69-54a5a21c91d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ler dados Camada Silver\n",
    "Filtrado pela maior data na tabela fato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed422817-9bbb-4677-9cec-460924d1fe2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_sub, lit\n",
    "\n",
    "# Ler a maior data de venda da tabela fato_vendas\n",
    "max_data_venda = spark.read.format(\"delta\").load(gold_fato_path) \\\n",
    "                          .selectExpr(\"max(DataVenda) as MaxDataVenda\") \\\n",
    "                          .collect()[0][\"MaxDataVenda\"]\n",
    "\n",
    "display(max_data_venda)\n",
    "\n",
    "# Carregar dados da Silver filtrando pela DataVenda maior que a obtida acima\n",
    "df_silver = spark.read.format(\"parquet\").load(silver_path) \\\n",
    "                          .filter(f\"Data > '{max_data_venda}'\")\n",
    "\n",
    "df_silver.count()                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae2901bb-159e-4c49-a35a-5794d4147069",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Criação da Dimensão Produto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cd61be0-b1b8-4087-bbce-48a15691d94a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, current_timestamp\n",
    "#Nome tabela destino\n",
    "\n",
    "tb_destino = \"dim_produto\"\n",
    "\n",
    "# Extrair produtos únicos para a dimensão Produto\n",
    "dim_produto_df = df_silver.select(\n",
    "    \"IDProduto\", \"Produto\", \"Categoria\").dropDuplicates()\n",
    "\n",
    "# Adicionar chave substituta (surrogate keys)\n",
    "dim_produto_df = dim_produto_df.withColumn(\"sk_produto\", monotonically_increasing_id()+1) \\\n",
    "                               .withColumn(\"data_atualizacao\", current_timestamp())\n",
    "\n",
    "\n",
    "# Escrever DimProduto no formato Delta\n",
    "dim_produto_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(f\"{gold_path}/{tb_destino}\")\n",
    "display(dim_produto_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bba96d39-f6e9-4971-8b77-3c9475761626",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Criação da Dimensão Categoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76509140-b502-48ba-9f5e-24844a45ea34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "#Nome tabela destino\n",
    "\n",
    "tb_destino = \"dim_categoria\"\n",
    "\n",
    "# Extrair Categorias únicas para a dimensão Categoria\n",
    "dim_categoria_df = df_silver.select(\n",
    "    \"Categoria\").dropDuplicates()\n",
    "\n",
    "# Adicionar chave substituta (surrogate keys)\n",
    "dim_categoria_df = dim_categoria_df.withColumn(\"sk_categoria\", monotonically_increasing_id()+1)\\\n",
    "                                   .withColumn(\"data_atualizacao\", current_timestamp())\n",
    "\n",
    "# Escrever DimCatgoria no formato Parquet, particionando por Categoria\n",
    "dim_categoria_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(f\"{gold_path}/{tb_destino}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1842462d-3602-45ab-889b-55c8e53d24c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Criação da Dimensão Segmento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5140696a-4139-4159-8a3a-055d38ece6b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Nome tabela destino\n",
    "\n",
    "tb_destino = \"dim_segmento\"\n",
    "\n",
    "# Extrair Segmentos únicos para a dimensão Segmentos\n",
    "dim_segmento_df = df_silver.select(\n",
    "   \"Segmento\").dropDuplicates()\n",
    "\n",
    "# Adicionar chave substituta (surrogate keys)\n",
    "dim_segmento_df = dim_segmento_df.withColumn(\"sk_segmento\", monotonically_increasing_id()+1) \\\n",
    "                                 .withColumn(\"data_atualizacao\", current_timestamp())\n",
    "\n",
    "\n",
    "# Escrever DimSegmento no formato Parquet\n",
    "dim_segmento_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(f\"{gold_path}/{tb_destino}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df8cf69d-e3fa-45f9-a2da-c9c07258e526",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Criação da Dimensão Fabricante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebd8ae90-4dba-4cd1-a7e2-8ba19cd49657",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Nome tabela destino\n",
    "tb_destino = \"dim_fabricante\"\n",
    "\n",
    "# Extrair produtos únicos para a dimensão Fabricante    \n",
    "dim_fabricante_df = df_silver.select(\n",
    "    \"IDFabricante\", \"Fabricante\").dropDuplicates()\n",
    "\n",
    "# Adicionar chave substituta (surrogate keys)\n",
    "dim_fabricante_df = dim_fabricante_df.withColumn(\"sk_fabricante\", monotonically_increasing_id()+1)\\\n",
    "                                      .withColumn(\"data_atualizacao\", current_timestamp())\n",
    "\n",
    "\n",
    "# Escrever DimFabricante no formato Delta\n",
    "dim_fabricante_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(f\"{gold_path}/{tb_destino}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd5c9c48-3c05-4cfa-b858-0d98b843a440",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Criação da Dimensão Geografia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9cb6c3f-6543-441f-a304-c86ffb60ef5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Nome tabela destino\n",
    "tb_destino = \"dim_geografia\"\n",
    "\n",
    "# Extrair Geografia  únicos para a dimensão Geografia\n",
    "dim_geografia_df = df_silver.select(\n",
    "     \"Cidade\", \"Estado\", \"Regiao\", \"Distrito\", \"Pais\", \"CodigoPostal\"\n",
    ").dropDuplicates()\n",
    "\n",
    "# Adicionar chave substituta\n",
    "dim_geografia_df = dim_geografia_df.withColumn(\"sk_geografia\", monotonically_increasing_id()+1) \\\n",
    "                                   .withColumn(\"data_atualizacao\", current_timestamp())\n",
    "\n",
    "\n",
    "# Escrever DimGeografia no formato Parquet\n",
    "dim_geografia_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(f\"{gold_path}/{tb_destino}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30e39d75-c09d-4088-ac72-f5fe5a9332d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Criação da Dimensão Cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf439c04-ea5c-421c-9c76-e51123c47a79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Nome tabela destino\n",
    "tb_destino = \"dim_cliente\"\n",
    "\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id\n",
    "# Passo 1 - Extrair clientes únicos para a dimensão Cliente\n",
    "dim_cliente_df = df_silver.select(\n",
    "    \"IDCliente\", \"Nome\", \"Email\", \"Cidade\", \"Estado\", \"Regiao\", \"Distrito\", \"Pais\", \"CodigoPostal\"\n",
    ").dropDuplicates()\n",
    "\n",
    "# Passo 2 - Realizar o join para obter a SK_Geografia\n",
    "dim_cliente_com_sk_df = dim_cliente_df.alias(\"cliente\") \\\n",
    "    .join(dim_geografia_df.alias(\"geografia\"), \n",
    "          (col(\"cliente.Cidade\") == col(\"geografia.Cidade\")) &\n",
    "          (col(\"cliente.Estado\") == col(\"geografia.Estado\")) &\n",
    "          (col(\"cliente.Regiao\") == col(\"geografia.Regiao\")) &\n",
    "          (col(\"cliente.Distrito\") == col(\"geografia.Distrito\")) &\n",
    "          (col(\"cliente.Pais\") == col(\"geografia.Pais\")) &\n",
    "          (col(\"cliente.CodigoPostal\") == col(\"geografia.CodigoPostal\")), \n",
    "          \"left\") \\\n",
    "    .select(\"cliente.IDCliente\", \"cliente.Nome\", \"cliente.Email\", \"geografia.sk_geografia\")\n",
    "\n",
    "# Passo 3 - Adicionar chave substituta\n",
    "dim_cliente_com_sk_df = dim_cliente_com_sk_df.withColumn(\"sk_cliente\", monotonically_increasing_id()+1) \\\n",
    "                                             .withColumn(\"data_atualizacao\", current_timestamp())\n",
    "\n",
    "\n",
    "# Passo 4 - Selecionar colunas específicas\n",
    "dim_cliente_com_sk_df = dim_cliente_com_sk_df.select(\"IDCliente\", \"Nome\",\"Email\", \"sk_geografia\", \"sk_cliente\",\"data_atualizacao\")\n",
    "\n",
    "# Passo 5 - Escrever DimCliente no formato Delta\n",
    "dim_cliente_com_sk_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(f\"{gold_path}/{tb_destino}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1894f40d-60d7-46e9-bd54-69fe71351f2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Criação de Tabela Fato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37b8b3eb-b651-4f33-a31c-7003a1285f04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Nome tabela destino\n",
    "tb_destino = \"fato_vendas\"\n",
    "\n",
    "from pyspark.sql.functions import broadcast,year, month\n",
    "# Juntar dados da Silver com tabelas de dimensões para obter as chaves substitutas\n",
    "fato_vendas_df = df_silver.alias(\"s\") \\\n",
    "    .join(broadcast(dim_produto_df.select(\"IDProduto\", \"sk_produto\").alias(\"dprod\")), \"IDProduto\") \\\n",
    "    .join(broadcast(dim_categoria_df.select(\"Categoria\", \"sk_categoria\").alias(\"dcat\")), \"Categoria\") \\\n",
    "    .join(broadcast(dim_segmento_df.select(\"Segmento\", \"sk_segmento\").alias(\"dseg\")), \"Segmento\") \\\n",
    "    .join(broadcast(dim_fabricante_df.select(\"Fabricante\", \"sk_fabricante\").alias(\"dfab\")), \"Fabricante\") \\\n",
    "    .join(broadcast(dim_cliente_com_sk_df.select(\"IDCliente\", \"sk_cliente\").alias(\"dcli\")), \"IDCliente\") \\\n",
    "    .select(\n",
    "        col(\"s.Data\").alias(\"DataVenda\"),\n",
    "        \"sk_produto\",\n",
    "        \"sk_categoria\",\n",
    "        \"sk_segmento\",\n",
    "        \"sk_fabricante\",\n",
    "        \"sk_cliente\",\n",
    "        \"Unidades\",\n",
    "        col(\"s.PrecoUnitario\"),\n",
    "        col(\"s.CustoUnitario\"),\n",
    "        col(\"s.TotalVendas\"),\n",
    "        current_timestamp().alias(\"data_atualizacao\")\n",
    "    )\n",
    "\n",
    "# Escrever tabela Fato no formato Delta, particionando por DataVenda (ano e mês)\n",
    "fato_vendas_df.withColumn(\"Ano\", year(\"DataVenda\")) \\\n",
    "             .withColumn(\"Mes\", month(\"DataVenda\")) \\\n",
    "             .write.format(\"delta\") \\\n",
    "             .mode(\"append\")\\\n",
    "             .option(\"mergeSchema\", \"true\")\\\n",
    "             .option(\"MaxRecordsPerFile\", 1000000)\\\n",
    "             .partitionBy(\"Ano\", \"Mes\")\\\n",
    "             .save(f\"{gold_path}/{tb_destino}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "934e4510-bdda-4d01-805d-a3ab49c63b06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Demonstração de informação total vendas por ano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2b06b72-290a-495b-a785-3389aa5f86f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, col\n",
    "gold_path = \"/mnt/lhdw/gold/vendas_delta/\"\n",
    "# Consulta da fato vendas por categoria ano a ano com a soma de total de vendas\n",
    "\n",
    "resultado = spark.read.format(\"delta\").load(f\"{gold_path}/fato_vendas\") \\\n",
    "    .groupBy(\"Ano\") \\\n",
    "    .agg(sum(\"TotalVendas\").alias(\"SomaTotalVendas\")) \\\n",
    "    .orderBy(col(\"Ano\"), col(\"SomaTotalVendas\").desc())\n",
    "\n",
    "display(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "907ddce9-46b7-43d6-9975-e831e1e0f4d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c47ecd5e-a69c-490b-9dc0-52c5ef5762aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Limpeza de Memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27462067-a7da-4f34-b982-e310dc50e3f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "# Coletar lixo após operações pesadas para liberar memória\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "005 Load Gold Delta Incremental",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
