{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e2f3cae-8a78-44e3-8715-30af1c948916",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1. Configurações Iniciais e Importações**\n",
    "\n",
    "Aqui está um exemplo de um notebook em PySpark para implementar a arquitetura Medallion com as camadas Bronze, Silver e Gold, utilizando Databricks e Delta Lake. Este exemplo segue as boas práticas de desenvolvimento e performance, incluindo a criação de surrogate keys (chaves substitutas) para as dimensões e otimização da tabela de fatos na camada Gold.\n",
    "\n",
    "**Explicações:**\n",
    "\n",
    "- Importar bibliotecas e funções necessárias.\n",
    "- Definir os caminhos de arquivo para as camadas Bronze, Silver e Gold.\n",
    "- Configurar as definições do Spark para um desempenho ótimo, como partições de shuffle automático."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ad31a5b-9827-422b-88ab-5b614cbc0215",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importar as bibliotecas necessárias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Iniciar a SparkSession com configurações otimizadas.\n",
    "# Define um número fixo de partições para shuffle, melhorando o paralelismo                 \n",
    "# Define o tamanho máximo de partições para evitar muitos arquivos pequenos        \n",
    "# Usa o codec Snappy para compressão rápida, otimizando tempo de leitura e escrita    \n",
    "# Habilita otimizações adaptativas, ajustando o número de partições dinamicamente com base no tamanho dos dados\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Load Data Bronze\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\")  \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128MB\") \\\n",
    "    .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Definir caminhos de armazenamento no Data Lake\n",
    "lz_path_in = \"/Volumes/workspace/store/landingzone/vendas/processar\"\n",
    "lz_path_out = \"/Volumes/workspace/store/landingzone/vendas/processado\"\n",
    "bronze_path = \"/Volumes/workspace/store/bronze/vendas\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "750c76db-c5ad-49eb-b389-8fcd94d6a31c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Justificativa:**\n",
    "\n",
    "- **spark.sql.shuffle.partitions**: Define o número de partições para operações que envolvem shuffle (como joins e agregações). Escolher um valor fixo, como 200, garante que o cluster trabalhe de forma paralela de maneira eficiente.\n",
    "\n",
    "Um cálculo comum para o número de partições é o seguinte:\n",
    "\n",
    "_`número de partições = número de núcleos de CPU * 2 ou 3`_\n",
    "\n",
    "Isso ajuda a garantir que o Spark use todos os núcleos disponíveis.\n",
    "- **spark.sql.files.maxPartitionByte**s: Definimos o tamanho máximo dos arquivos particionados para evitar a criação de muitos arquivos pequenos, o que prejudicaria a performance de leitura e escrita.\n",
    "- **spark.sql.parquet.compression.codec**: Snappy é uma escolha comum para Parquet, pois oferece uma boa combinação de compressão rápida e descompressão eficiente.\n",
    "- **spark.sql.adaptive.enabled**: A otimização adaptativa ajusta o plano de execução conforme o tamanho dos dados, melhorando o desempenho automaticamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14e168d7-e53d-4d8f-a5f3-6e97309c68db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "**2. Camada Bronze: Ingestão de Dados Brutos**\n",
    "\n",
    "A camada Bronze armazena dados brutos com formato parquet, sem transformações significativas. Aqui vamos simplesmente gravar os dados brutos como parquet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb267a36-de49-4703-8324-3f69ae10433b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Criando um Schema para dados brutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18dd4e9d-39e3-4115-bf93-7db439834990",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Definir o esquema dos dados brutos\n",
    "schema_lz = StructType([\n",
    "    StructField(\"IDProduto\", IntegerType(), True),\n",
    "    StructField(\"Data\", DateType(), True),\n",
    "    StructField(\"IDCliente\", IntegerType(), True),\n",
    "    StructField(\"IDCampanha\", IntegerType(), True),\n",
    "    StructField(\"Unidades\", IntegerType(), True),\n",
    "    StructField(\"Produto\", StringType(), True),\n",
    "    StructField(\"Categoria\", StringType(), True),\n",
    "    StructField(\"Segmento\", StringType(), True),\n",
    "    StructField(\"IDFabricante\", IntegerType(), True),\n",
    "    StructField(\"Fabricante\", StringType(), True),\n",
    "    StructField(\"CustoUnitario\", DoubleType(), True),\n",
    "    StructField(\"PrecoUnitario\", DoubleType(), True),\n",
    "    StructField(\"CodigoPostal\", StringType(), True),\n",
    "    StructField(\"EmailNome\", StringType(), True),\n",
    "    StructField(\"Cidade\", StringType(), True),\n",
    "    StructField(\"Estado\", StringType(), True),\n",
    "    StructField(\"Regiao\", StringType(), True),\n",
    "    StructField(\"Distrito\", StringType(), True),\n",
    "    StructField(\"Pais\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Leitura dos dados e adição da coluna nome do arquivo durante a leitura\n",
    "df_vendas = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema_lz) \\\n",
    "    .csv(lz_path_in) \\\n",
    "    .withColumn(\n",
    "        \"filename\",\n",
    "        regexp_extract(col(\"_metadata.file_path\"), \"([^/]+)$\", 1)\n",
    "    )\n",
    "\n",
    "distinct_filenames = df_vendas.select(\"filename\").distinct()\n",
    "\n",
    "# Exibindo o DataFrame para verificar a leitura correta dos dados\n",
    "display(df_vendas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a7a9dc3-e288-47d4-bc01-3177e4c2b3cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Apresentando os arquivos lidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "205e788b-d52b-4197-9602-93113d8d5206",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "display(distinct_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca27873b-0e53-4b03-b1a0-2e9493c1e0f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Salvar/Persistir dados na camada Bronze Bronze\n",
    "\n",
    "Os dados serão salvos de forma particionada **Ano e Mês**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e64fda6c-3cc6-4f68-a1e0-914be698294a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Escrever a tabela no formato Parquet, particionando por DataVenda (ano e mês)\n",
    "df_vendas \\\n",
    "    .withColumn(\"Ano\", year(\"Data\")) \\\n",
    "    .withColumn(\"Mes\", month(\"Data\")) \\\n",
    "    .write.mode(\"overwrite\").partitionBy(\"Ano\", \"Mes\").parquet(bronze_path)\n",
    "\n",
    "# Apresentando o DataFrame\n",
    "display(df_vendas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cee8ea4-d57e-4db2-afc9-a76a102dd2a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Justificativas:**\n",
    "\n",
    "- Lê os dados brutos a partir de um arquivo CSV na landing zone e escreve esses dados no formato Parquet na camada Bronze.\n",
    "- O Parquet é escolhido pelo seu suporte a colunas e sua eficiência tanto em termos de espaço quanto em desempenho de leitura e escrita."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d18d8c17-2395-414e-843a-b081d58104d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Mover os arquivos processados para pasta processado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5ddc8c7-e1fb-4c3d-8bbf-122c757b1269",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mover os arquivos processados para o caminho lz_path_out\n",
    "# Nota: A operação de mover arquivos diretamente não é suportada pelo DataFrame API do Spark. É necessário utilizar o dbutils.fs.mv para mover os arquivos manualmente após o processamento.\n",
    "# Primeiro, verifique se há arquivos a serem movidos\n",
    "if distinct_filenames.select(\"filename\").distinct().count() > 0:\n",
    "    filenames = distinct_filenames.select(\"filename\").distinct().collect()\n",
    "\n",
    "    for row in filenames:\n",
    "        src_path = row.filename\n",
    "        dbutils.fs.mv(lz_path_in + \"/\" + src_path, lz_path_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cd0776b-b479-4bec-921a-1b83c1854ab9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Evidências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a05dd97c-3748-42e2-83ff-74b223bd13f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls /Volumes/workspace/store/landingzone/vendas/processar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "968556e1-9d6b-43c4-9de3-7043caa2cdb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls /Volumes/workspace/store/landingzone/vendas/processado/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a7cfe04-6953-474f-ac1a-b9c105fa1f6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls /Volumes/workspace/store/bronze/vendas/Ano=2012/Mes=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4543d0e5-2408-40c6-bde6-398e1e836dac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### A opção de gravar dados no modo \"append\" \n",
    "\n",
    "Permite adicionar novos dados a um arquivo existente, sem substituir ou excluir os dados já presentes. \n",
    "\n",
    "No caso específico do código fornecido, a linha de código comentada `df_vendas.withColumn(\"Ano\", year(\"Data\")) \\ .withColumn(\"Mes\", month(\"Data\")) \\ .write.mode(\"append\").partitionBy(\"Ano\", \"Mes\").parquet(bronze_path)` indica que os dados do DataFrame `df_vendas` serão adicionados ao arquivo Parquet existente no caminho `bronze_path`, mantendo a estrutura de particionamento por ano e mês.\n",
    "\n",
    "Essa opção é útil quando se deseja adicionar novos dados a um conjunto de dados já existente, como por exemplo, quando novas vendas são registradas e precisam ser incorporadas ao conjunto de dados de vendas existente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d16d1eb9-da44-4329-9798-456593bb8ebf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df_vendas.withColumn(\"Ano\", year(\"Data\")) \\\n",
    "#         .withColumn(\"Mes\", month(\"Data\")) \\\n",
    "#         .write.mode(\"append\").partitionBy(\"Ano\", \"Mes\").parquet(bronze_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30c5c5e0-2539-442f-98fb-251eac06e787",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Gerenciar o uso de memória \n",
    "Em PySpark, é importante gerenciar o uso de memória eficientemente, especialmente quando se trabalha com grandes conjuntos de dados. Para isso, você pode usar alguns comandos específicos que ajudam a liberar memória, remover objetos em cache ou persistidos e forçar a coleta de lixo.\n",
    "\n",
    "**1. Limpar cache:**\n",
    "PySpark armazena dados em cache para melhorar o desempenho de operações repetidas. Para liberar esses dados, você pode usar o comando unpersist()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9da8ff60-496a-40cf-a5d2-757626cfc695",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exemplo de como liberar o cache de um DataFrame\n",
    "# OBS: Não funciona em ambientes SERVERLESS\n",
    "\n",
    "df_vendas.unpersist()\n",
    "\n",
    "# O comando unpersist() remove o DataFrame do cache, liberando a memória associada. Ele é especialmente útil quando você já não precisa mais dos dados persistidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "171f1a11-ce2a-4548-a88f-293e61c54b37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**2. Limpar todos os dados em cache:**\n",
    "\n",
    "Se houver vários DataFrames em cache, você pode limpá-los todos de uma vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f0835e4-ccb0-4b65-bb6d-5007a69215d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Limpar todos os dados em cache\n",
    "\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "# clearCache() limpa o cache de todos os objetos em cache no SparkSession atual, liberando uma quantidade significativa de memória quando múltiplos DataFrames estão sendo reutilizados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e5551dd-9078-4ae4-a090-089d7bd8854f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**3. Forçar coleta de lixo:**\n",
    "\n",
    "O Python possui um coletor de lixo que remove objetos não referenciados da memória. Você pode forçar a coleta de lixo para liberar memória."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45b903f2-79b6-47a5-b274-be1e3fe23a28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "#Comentário: Esse comando força o coletor de lixo a executar imediatamente, liberando a memória de objetos Python que não estão mais em uso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e56b7c34-923d-4b34-ae8a-cb96228739b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**4. Liberar variáveis manualmente:**\n",
    "\n",
    "Se você criou variáveis grandes que não são mais necessárias, você pode removê-las explicitamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fc418b7-98de-41ce-8c77-a222be2977b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "del df_vendas\n",
    "\n",
    "# O comando del remove o objeto da memória. Isso é útil quando você tem grandes DataFrames ou objetos Python que já não são necessários."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a22a9603-9c12-47ae-b836-9e8091177a62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "**Dicas adicionais:**\n",
    "- Evite cachear DataFrames desnecessários.\n",
    "\n",
    "**Resumo**\n",
    "\n",
    "- **Para uma limpeza rápida e geral**: Use spark.catalog.clearCache().\n",
    "- **Para liberar memória de DataFrames específicos**: Use df.unpersist().\n",
    "- **Para remover variáveis específicas**: Use del.\n",
    "- **Para uma solução completa**: Reinicie o cluster."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5232872374287403,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "002 Load Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
