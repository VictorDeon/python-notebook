{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "451c49b8-fbc8-4664-a602-cab84d85088a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Arquivos Parquet\n",
    "\n",
    "Os arquivos Parquet são um formato de armazenamento colunar amplamente utilizado em ambientes de Big Data, como o Databricks e o Apache Spark. Eles oferecem alta eficiência de compressão e desempenho em consultas analíticas, tornando-se ideais para processamento e análise de grandes volumes de dados.\n",
    "\n",
    "* 80% (Aprox) menos armazenamento ao comparar com CSV e Json\n",
    "\n",
    "* 30x (Aprox) Mais Rápido\n",
    "\n",
    "* 90% (Aprox) Economia na leitura agiliza o cluster isso resulta em leituras e gravações mais rápidas.\n",
    "\n",
    "* Pode ser particionado\n",
    "\n",
    "* Quanto menor tamando de armazenamento menos dinheiro a empresa paga em recursos Cloud\n",
    "\n",
    "##### Documentação Databricks\n",
    "\n",
    "https://docs.databricks.com/pt/external-data/parquet.html\n",
    "\n",
    "##### Documentação Spark\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-data-sources-parquet.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d8c71ee-7fc7-4f7f-a248-374aff59c842",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Transformando DataFlame em Parquet\n",
    "\n",
    "O formato Parquet é amplamente utilizado para armazenar dados de forma eficiente em ambientes de Big Data. Transformar um DataFrame em Parquet permite aproveitar benefícios como compressão, leitura otimizada e facilidade de integração com ferramentas analíticas, tornando o processamento de grandes volumes de dados mais rápido e econômico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc0d917e-8229-4547-bce3-2a0883c5d2ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = \"/Volumes/workspace/default/tutorial/anac\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e28a75e3-8fa4-4e40-bcca-6930173bb3cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(f\"{path}/V_OCORRENCIA_AMPLA.json\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "352756c3-6ef3-43ed-8767-c410197163ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write \\\n",
    "  .format('parquet') \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save(f\"{path}/V_OCORRENCIA_AMPLA_PARQUET\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca44cb28-1e81-41b4-9fd3-1f4cc88f2396",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(f\"{path}/V_OCORRENCIA_AMPLA_PARQUET\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7f544a7-55bc-406c-9563-07b7ef6b38e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# O CSV tem 9MB, JSON tem 20MB e o PARQUET ficou com 3MB\n",
    "def get_folder_size(path):\n",
    "    files = dbutils.fs.ls(path)\n",
    "    return sum(f.size for f in files if not f.isDir())\n",
    "\n",
    "folder_size = get_folder_size(f\"{path}/V_OCORRENCIA_AMPLA.csv\")\n",
    "print(f\"Tamanho total da pasta CSV: {folder_size} bytes\")\n",
    "\n",
    "folder_size = get_folder_size(f\"{path}/V_OCORRENCIA_AMPLA.json\")\n",
    "print(f\"Tamanho total da pasta JSON: {folder_size} bytes\")\n",
    "\n",
    "folder_size = get_folder_size(f\"{path}/V_OCORRENCIA_AMPLA_PARQUET\")\n",
    "print(f\"Tamanho total da pasta PARQUET: {folder_size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9da11085-106f-4228-91d7-3633cb4d1769",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## lendo arquivo Parquet\n",
    "\n",
    "A leitura de arquivos Parquet é uma etapa fundamental em pipelines de dados, pois permite acessar informações armazenadas de forma eficiente e otimizada. O formato Parquet, por ser colunar e suportar compressão, proporciona consultas rápidas e uso reduzido de recursos computacionais, sendo amplamente adotado em ambientes de Big Data como o Databricks e o Apache Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c0a508b-3ad0-46af-a519-f52a15ce7ad5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfpq = spark.read.parquet(f\"{path}/V_OCORRENCIA_AMPLA_PARQUET\")\n",
    "display(dfpq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb86efc1-2c9f-4cc2-8230-c66b85f7a4c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Salvando parquet com Compactado\n",
    "\n",
    "O armazenamento de dados em formato Parquet pode ser ainda mais eficiente ao aplicar compactação. A compactação reduz o espaço ocupado em disco e pode melhorar o desempenho de leitura e escrita, especialmente em grandes volumes de dados. Utilizar Parquet compactado é uma prática recomendada em ambientes de Big Data para otimizar custos e recursos computacionais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5faf44a-8e8e-44d0-8a8d-bd2cc5cf325d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write \\\n",
    "  .format('parquet') \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .option(\"compression\", \"gzip\") \\\n",
    "  .save(f\"{path}/V_OCORRENCIA_AMPLA_PARQUET.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb481bc4-2e45-41c2-b155-6d3d44211fe9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52a2ad75-c41d-4869-b060-09a33f9149b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(f\"{path}/V_OCORRENCIA_AMPLA_PARQUET.zip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95086071-9506-468c-bf0c-a43eca44dffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# O CSV tem 2MB, JSON tem 3MB e o PARQUET ficou com 3MB e o PARQUET ZIP 2MB\n",
    "def get_folder_size(path):\n",
    "    files = dbutils.fs.ls(path)\n",
    "    return sum(f.size for f in files if not f.isDir())\n",
    "\n",
    "folder_size = get_folder_size(f\"{path}/V_OCORRENCIA_AMPLA_CSV.zip\")\n",
    "print(f\"Tamanho total da pasta CSV ZIP: {folder_size} bytes\")\n",
    "\n",
    "folder_size = get_folder_size(f\"{path}/V_OCORRENCIA_AMPLA_JSON.zip\")\n",
    "print(f\"Tamanho total da pasta JSON ZIP: {folder_size} bytes\")\n",
    "\n",
    "folder_size = get_folder_size(f\"{path}/V_OCORRENCIA_AMPLA_PARQUET\")\n",
    "print(f\"Tamanho total da pasta PARQUET: {folder_size} bytes\")\n",
    "\n",
    "folder_size = get_folder_size(f\"{path}/V_OCORRENCIA_AMPLA_PARQUET.zip\")\n",
    "print(f\"Tamanho total da pasta PARQUET ZIP: {folder_size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4df4d384-e5f1-4db2-9885-c9088903899b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Lendo Arquivo Parquet compactado\n",
    "\n",
    "A leitura de arquivos Parquet compactados é essencial para otimizar o armazenamento e o desempenho em ambientes de Big Data. O Parquet, aliado à compactação, permite acessar grandes volumes de dados de forma eficiente, reduzindo custos de armazenamento e acelerando operações de leitura e análise. Essa abordagem é amplamente utilizada em plataformas como Databricks e Apache Spark para garantir escalabilidade e performance em pipelines de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b62222c1-36c1-4582-a6f9-a531a65afa1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfParquetZip = spark.read\\\n",
    "    .format(\"parquet\")\\\n",
    "    .option(\"compression\", \"gzip\")\\\n",
    "    .load(f\"{path}/V_OCORRENCIA_AMPLA_PARQUET.zip\")\n",
    "\n",
    "display(dfParquetZip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08adb06f-05d0-4f2b-a099-789a3323666a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## tempo de Execução parquet x json\n",
    "\n",
    "A escolha do formato de armazenamento de dados impacta diretamente o desempenho e a eficiência de pipelines analíticos. Parquet e JSON são formatos amplamente utilizados, porém apresentam diferenças significativas em termos de velocidade de leitura, escrita e uso de recursos. Nesta seção, vamos comparar o tempo de execução entre operações com arquivos Parquet e JSON, destacando as vantagens do formato colunar Parquet em cenários de Big Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90a14761-37b3-4561-84ce-b4ca04d09868",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Json 2.09s\n",
    "df_json = spark.read.json(f\"{path}/V_OCORRENCIA_AMPLA.json\")\n",
    "display(df_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c27776ee-fb58-43ff-a166-d747a93e2c3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# parquet 1.71s\n",
    "dfpq = spark.read.parquet(f\"{path}/V_OCORRENCIA_AMPLA_PARQUET\")\n",
    "display(dfpq)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "08 - Arquivo Parquet",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
