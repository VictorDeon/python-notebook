{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39cbba60-cae6-4ac2-a5ab-69778f7a0870",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Tipos de Dados\n",
    "\n",
    "Referencia de Documetação https://spark.apache.org/docs/latest/api/python/reference/index.html\n",
    "\n",
    "Nesta seção, vamos explorar os principais tipos de dados utilizados no Spark, fundamentais para manipulação, análise e transformação de dados em ambientes distribuídos. Compreender esses tipos é essencial para garantir operações eficientes e corretas ao trabalhar com DataFrames e outras estruturas de dados no Spark.\n",
    "\n",
    "* **nullable**: isso indica que alguns dos registros nessa coluna podem ter valores nulos.\n",
    "\n",
    "* **string ou varchar**: Texto.\n",
    "\n",
    "* **integer ou int**: Numero inteiro.\n",
    "\n",
    "* **long**: Numero inteiro longo.\n",
    "\n",
    "* **double**: precisão dupla.\n",
    "\n",
    "* **float**: precisão simples.\n",
    "\n",
    "* **decimal**: decimal.\n",
    "\n",
    "* **timestamp**: Data e hora.\n",
    "\n",
    "* **date**: Somente a Data.\n",
    "\n",
    "* **boolean**: booleana (True e False) 0=False e 1=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b85ad422-76de-429c-86cf-756f144841ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = \"/Volumes/workspace/default/tutorial\"\n",
    "df = spark.read.json(f\"{path}/anac/V_OCORRENCIA_AMPLA.json\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feb61625-e2d7-4534-9e1e-83d50447a53f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orders_df = spark.read. \\\n",
    "    format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(f\"{path}/bike-store/orders.csv\")\n",
    "\n",
    "display(orders_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f24c1e2-6c3d-47fc-99e6-80941cf26e22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8355a4b5-4610-4d25-8c5b-580d29cf4d4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ver nomes das colunas \n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9af79c3-0633-4013-9905-834499b7ebe9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for Loop in df.columns:\n",
    "    print(Loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2e20950-a045-4d1c-965e-2c33eb1902dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Selecionando apenas algumas colunas do DF\n",
    "\n",
    "Ao trabalhar com DataFrames no Spark, muitas vezes é necessário selecionar apenas um subconjunto de colunas relevantes para a análise ou transformação desejada. Essa seleção permite otimizar o processamento, facilitar a visualização dos dados e preparar o DataFrame para etapas subsequentes do pipeline de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46fbc881-fd70-4b00-8af8-c5806300b479",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df['Classificacao_da_Ocorrência','Danos_a_Aeronave','UF','Municipio','Ilesos_Passageiros'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53e49489-9aac-480a-b9f4-e16380171bb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Salvando em um novo Df\n",
    "Filtrado = df['Classificacao_da_Ocorrência','Danos_a_Aeronave','UF','Municipio','Ilesos_Passageiros']\n",
    "display(Filtrado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccb8f329-7340-4bc2-aa6b-7f577b3820c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "958dbcf0-4913-4dd5-8713-2f115c9a6829",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Criando uma variavel com as colunas (lista)\n",
    "ColunasSelecionadas = [\n",
    "    'Categoria_da_Aeronave',\n",
    "    'Classificacao_da_Ocorrência',\n",
    "    'Danos_a_Aeronave',\n",
    "    'Data_da_Ocorrencia',\n",
    "    'Descricao_do_Tipo',\n",
    "    'Fase_da_Operacao'\n",
    "]\n",
    "\n",
    "display(df[ColunasSelecionadas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ad99a37-45fb-441e-9bfc-4de18087446a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Salvando em um Df \n",
    "ColunasSelecionadas = [\n",
    "    'Categoria_da_Aeronave',\n",
    "    'Classificacao_da_Ocorrência',\n",
    "    'Danos_a_Aeronave',\n",
    "    'Data_da_Ocorrencia',\n",
    "    'Descricao_do_Tipo',\n",
    "    'Fase_da_Operacao'\n",
    "]\n",
    "\n",
    "Novo = df[ColunasSelecionadas]\n",
    "display(Novo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f5b2876-ba31-46fc-ba87-b44537272fbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Método Select\n",
    "display(df.select('Danos_a_Aeronave','Data_da_Ocorrencia','Operador', 'UF') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "668b13c8-4024-4a7e-afa6-095891a0f167",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Criando novas Colunas\n",
    "\n",
    "Ao trabalhar com DataFrames no Spark, frequentemente é necessário criar novas colunas a partir de transformações, cálculos ou condições aplicadas sobre os dados existentes. Essa prática é fundamental para enriquecer o conjunto de dados, facilitar análises e preparar informações para etapas posteriores do pipeline de dados. Nesta seção, veremos como adicionar novas colunas utilizando diferentes abordagens no Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c94b082-1020-40c2-be58-1114b5e1b33f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "teste = df['Classificacao_da_Ocorrência','UF','Municipio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68274213-ccbc-4975-bcb2-cf46fc91e56a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f5afde0-29f6-41ce-9b5e-539522512246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "display(teste.withColumn(\"TesteTexto\",lit(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e424f7ca-ec38-42a8-a6ec-284cbf04dfcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Nova coluna com minicipio - UF \n",
    "# Lit = Trazer o valor literal para a coluna ex uma coluna com todas as linhas com o valor 10 ou com a palava Brasil\n",
    "\n",
    "from pyspark.sql.functions import concat, lit\n",
    "display(teste.withColumn(\"Municipio_UF\", concat(teste.Municipio, lit(\" - \"), teste[\"UF\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3340e953-2b82-469b-9c35-0df5311ed5b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Salvando em Df\n",
    "from pyspark.sql.functions import concat, lit\n",
    "Tratado = teste.withColumn(\"Municipio_UF\", concat(teste.Municipio, lit(\" - \"),df[\"UF\"]))\n",
    "display(Tratado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9caf5d4-0a8f-4ceb-92e7-171dacace09c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inserindo mais de uma coluna no mesmo comando exemplo uma coluna com país, outra com Municipio em letra miniscula\n",
    "from pyspark.sql.functions import concat,lit,lower\n",
    "Tratado2 = teste\\\n",
    "    .withColumn(\"Municipio_UF\", concat(teste.Municipio, lit(\" - \"), df[\"UF\"])) \\\n",
    "    .withColumn(\"País\", lit(\"Brasil\")) \\\n",
    "    .withColumn(\"Minisculo\", lower(teste.Municipio))\n",
    "\n",
    "display(Tratado2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25c3cea3-5a71-467a-a802-0c60527f1844",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Criando Coluna Condicional\n",
    "\n",
    "Ao manipular dados em DataFrames no Spark, muitas vezes é necessário criar colunas cujos valores dependem de condições específicas aplicadas sobre os dados existentes. As colunas condicionais permitem enriquecer o conjunto de dados com informações derivadas de regras de negócio, facilitando análises e tomadas de decisão. Nesta seção, veremos como implementar colunas condicionais de forma eficiente utilizando as funcionalidades do Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f23643a0-0b46-4626-bcb3-e70962616037",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Regiao = df['Danos_a_Aeronave', 'UF']\n",
    "display(Regiao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fa22dde-1231-4dbb-b25a-a40d9387b770",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Estado \\ Regiao \n",
    "\"\"\"\n",
    "'AC': 'Norte',\n",
    "'AL': 'Nordeste',\n",
    "'AP': 'Norte',\n",
    "'AM': 'Norte',\n",
    "'BA': 'Nordeste',\n",
    "'CE': 'Nordeste',\n",
    "'ES': 'Sudeste',\n",
    "'GO': 'Centro-Oeste',\n",
    "'MA': 'Nordeste',\n",
    "'MT': 'Centro-Oeste',\n",
    "'MS': 'Centro-Oeste',\n",
    "'MG': 'Sudeste',\n",
    "'PA': 'Norte',\n",
    "'PB': 'Nordeste',\n",
    "'PR': 'Sul',\n",
    "'PE': 'Nordeste',\n",
    "'PI': 'Nordeste',\n",
    "'RJ': 'Sudeste',\n",
    "'RN': 'Nordeste',\n",
    "'RS': 'Sul',\n",
    "'RO': 'Norte',\n",
    "'RR': 'Norte',\n",
    "'SC': 'Sul',\n",
    "'SP': 'Sudeste',\n",
    "'SE': 'Nordeste',\n",
    "'TO': 'Norte'\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaba9a01-0b89-49d3-95aa-e864500337ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "Regiao = Regiao \\\n",
    "    .withColumn(\n",
    "        \"Regiao\",\n",
    "        when(df.UF == \"MG\", \"Sudeste\") \\\n",
    "        .otherwise(\"Outra Regiao\")\n",
    "    )\n",
    "\n",
    "display(Regiao)\n",
    "#When = quando \n",
    "#otherwise = outra Forma (senão)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "742e623e-d93b-4fd8-b760-6bc02f9e5afe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mais de uma condição isin - esta em (Estra dentro de algo).\n",
    "# Muito usada para verificar se os valores de uma coluna ou expressão estão contidos em um conjunto de valores especificados\n",
    "from pyspark.sql.functions import when\n",
    "Regiao = Regiao\\\n",
    "    .withColumn(\n",
    "        \"Regiao\",\n",
    "        when(df.UF.isin (\"MG\",\"SP\",\"RJ\",\"ES\"), \"Sudeste\") \\\n",
    "        .otherwise(\"Outra Regiao\")\n",
    "    )\n",
    "\n",
    "display(Regiao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "782aa166-78b8-4bfa-a705-03b8348d58ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fazendo o mesmo Para regiao Sul [PR, RS, SC]\n",
    "from pyspark.sql.functions import when\n",
    "Regiao = Regiao \\\n",
    "    .withColumn(\n",
    "        \"Regiao\",\n",
    "        when(df.UF.isin (\"MG\",\"SP\",\"RJ\",\"ES\"), \"Sudeste\") \\\n",
    "        .when(df.UF.isin (\"PR\",\"RS\",\"SC\"), \"Sul\" ) \\\n",
    "        .otherwise(\"Outra Regiao\")\n",
    "    )\n",
    "\n",
    "display(Regiao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39a3ac21-875a-407b-863d-205b83615b8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Coluna Condicional de valor\n",
    "\n",
    "Ao trabalhar com DataFrames no Spark, é comum precisar criar colunas cujos valores são definidos a partir de condições específicas. Essas colunas condicionais permitem atribuir diferentes valores conforme regras de negócio ou critérios de análise, enriquecendo o conjunto de dados e facilitando a tomada de decisões. Nesta seção, veremos como criar colunas condicionais utilizando funções do Spark, tornando o processo flexível e eficiente para diferentes cenários."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8620ba6-d7f1-438d-98c6-7dff9eccfeae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Nova coluna com descrição Status de Ordem \n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "orders_df = orders_df \\\n",
    "    .withColumn(\"DescStatus\",\n",
    "        when(orders_df.order_status == 1 , \"Pendente\") \\\n",
    "        .otherwise(\"Sem Status\")\n",
    "    )\n",
    "\n",
    "display(orders_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1007d35f-5052-4efe-9099-df0dec82d6f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# inserindo Varias condições (Simulando Status de produção)\n",
    "# 1 = Pendente\n",
    "# 2 = Em produção\n",
    "# 3 = Em rota de entrega\n",
    "# 4 = Finalizado\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "orders_df = orders_df.withColumn(\n",
    "    \"DescStatus\",\n",
    "    when (orders_df.order_status == 1 , \"Pendente\") \\\n",
    "    .when (orders_df.order_status == 2 , \"Em produção\") \\\n",
    "    .when (orders_df.order_status == 3 , \"Em rota de entrega\") \\\n",
    "    .otherwise(\"Finalizado\")\n",
    ")\n",
    "\n",
    "display(orders_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b418d28-056c-4576-8a88-0337b19f4dea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "order_items = spark.read. \\\n",
    "    format(\"csv\")\\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(f\"{path}/bike-store/order_items.csv\")\n",
    "\n",
    "display(order_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7c9cc91-131d-44d0-bcf8-9422f0810d6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# caso for um valor especifico traga uma informação \n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "order_items = order_items.withColumn(\n",
    "    \"DescStatus\",\n",
    "    when(order_items.list_price == 599.99 , \"valor procurado\") \\\n",
    "    .otherwise(\"Não\")\n",
    ")\n",
    "\n",
    "display(order_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "778a7ee6-7de6-41bd-abc5-314a6b5f29bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Filtro do Valor procurado\n",
    "display(order_items.filter(order_items.DescStatus == \"valor procurado\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99c0cbe1-6820-4659-b877-eb4748af7ca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sumilado Com varios Valores\n",
    "# 0 a 500    = Barato\n",
    "# 501 a 1500 = Médio\n",
    "# > 1500     = Caro\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "Taxa = order_items.withColumn(\n",
    "    \"TaxaCliente\",\n",
    "    when((order_items.list_price >= 0) & (order_items.list_price <= 500), \"Barato\") \\\n",
    "    .when((order_items.list_price >= 501) & (order_items.list_price <= 1500), \"Médio\") \\\n",
    "    .when(order_items.list_price > 1500, \"Caro\") \\\n",
    "    .otherwise(\"Caro\")\n",
    ")\n",
    "\n",
    "display(Taxa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee92c76b-b21e-4cab-b55f-fc60fed08684",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Fazendo Filtro para validar \n",
    "display(Taxa.filter(Taxa.TaxaCliente == \"Caro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a222fef9-4db1-4869-9cd8-126bf623c3a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Comando Sql para Selecionar Colunas e Criar Novas\n",
    "\n",
    "Ao utilizar comandos SQL no Spark, é possível selecionar colunas específicas de um DataFrame e criar novas colunas diretamente na consulta. Essa abordagem facilita a manipulação e transformação dos dados, permitindo aplicar funções, expressões e condições para gerar colunas derivadas conforme a necessidade da análise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d9dfb14-bc24-4038-8717-93e9bc0935ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inserindo em uma tabela temporaria para \n",
    "df.createOrReplaceTempView(\"anac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3656526f-2e0a-49ac-ba04-a35c65398f66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79e1b3ae-d4c5-42ab-bb17-9fe282e85444",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Selecionando algumas colunas\n",
    "Novodf = spark.sql(\"\"\"\n",
    "select\n",
    "  Danos_a_Aeronave,\n",
    "  Municipio,\n",
    "  UF\n",
    "from anac\n",
    "\"\"\")\n",
    "\n",
    "display(Novodf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f383ffbd-e5df-41b9-8e8b-9e290d74cd40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Coluna condicional  \"MG\",\"SP\",\"RJ\",\"ES\" = \"Sudeste\"\n",
    "Novodf = spark.sql(\"\"\"\n",
    "select\n",
    "   Danos_a_Aeronave,\n",
    "   Municipio,\n",
    "   UF,\n",
    "   Case\n",
    "      when UF in ('MG','SP','RJ','ES') then 'Sudeste'\n",
    "      else 'Sem Regiao'\n",
    "      end as Regiao\n",
    "from anac\n",
    "\"\"\")\n",
    "\n",
    "display(Novodf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eda0157b-8002-4d18-bda4-f0e730e06bfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Coluna condicional  \"MG\",\"SP\",\"RJ\",\"ES\" =  \"Sudeste\" e 'PR',RS,SC = Sul \n",
    "\n",
    "Novodf = spark.sql(\"\"\"\n",
    "select\n",
    "   Danos_a_Aeronave,\n",
    "   Municipio,\n",
    "   UF,\n",
    "   Case\n",
    "      when UF in ('MG','SP','RJ','ES') then 'Sudeste'\n",
    "      when UF in ('PR','RS','SC') then 'Sul'\n",
    "      else 'Sem Regiao'\n",
    "      end as Regiao\n",
    "from anac \n",
    "\"\"\")\n",
    "\n",
    "display(Novodf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0038dd4-e9f4-4363-8acd-c6d3b5ec575d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(Novodf.filter(Novodf.Regiao == 'Sudeste'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1d53b24-f2c9-4344-b08e-04273211cd1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inserindo em tabela temporaria para rodar SQL\n",
    "order_items.createOrReplaceTempView(\"Vendas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a99a2fc1-fc9e-4fb0-ba57-50b120d5b253",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Consulta Para identificar tipo de cliente a partir da quantidade de compra \n",
    "Testevendas = spark.sql(\"\"\"\n",
    "select *, Case \n",
    "    when quantity = 1 then 'Mâo de Vaca'\n",
    "    else 'Top'\n",
    "    end TipoCliente\n",
    "from Vendas\n",
    "\"\"\")\n",
    "\n",
    "display(Testevendas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b11f48f-e7a1-4890-be1c-208ca7177e95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Classificação por faixa de Preço\n",
    "# 0 a 500    = Barato\n",
    "# 501 a 1500 = Médio\n",
    "# > 1500     = Caro\n",
    "\n",
    "Testevendas = spark.sql(\"\"\"\n",
    "select *, case \n",
    "    when list_price between   0 and 500 then 'Barato'\n",
    "    when list_price between 501 and 1500 then 'Médio'\n",
    "    when list_price > 1500 then 'Médio'\n",
    "    else 'Sem Perfil' \n",
    "    end as PerfilProduto\n",
    "from Vendas\n",
    "\"\"\")\n",
    "\n",
    "display(Testevendas)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "12 - Tipos de Dados",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
