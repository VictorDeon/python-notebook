{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "160e5382-b237-4673-b19b-4fee3c095827",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Filtro de dados no databricks\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.filter.html\n",
    "\n",
    "Filtrar dados é uma etapa fundamental na análise de dados, permitindo selecionar apenas as informações relevantes de um DataFrame. No Databricks, existem diversas formas de aplicar filtros utilizando métodos do PySpark, comandos SQL e funções específicas para manipulação de strings e valores. Nesta seção, vamos explorar as principais técnicas para filtrar dados de maneira eficiente e flexível.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c282ccce-d940-4362-bff8-5e9ef3b38dcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = \"/Volumes/workspace/default/tutorial/assets\"\n",
    "df = spark.read.json(f\"{path}/V_OCORRENCIA_AMPLA.json\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84b83204-eacb-43ed-8299-3bfc60b89137",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Filtros de texto (Filter)\n",
    "\n",
    "O método `.filter()` do PySpark DataFrame permite selecionar linhas que atendem a uma condição específica, retornando um novo DataFrame apenas com os dados filtrados. Ele aceita como parâmetro uma expressão booleana, que pode ser passada como uma string SQL ou como uma expressão construída com funções do PySpark. Os principais parâmetros são:\n",
    "\n",
    "- **condition**: Expressão booleana que define o critério de filtragem. Pode ser uma string (ex: `\"coluna = 'valor'\"`) ou uma expressão (`col(\"coluna\") == \"valor\"`).\n",
    "\n",
    "- **subset** *(opcional, em alguns contextos)*: Lista de colunas a serem consideradas na filtragem, geralmente usada em funções auxiliares.\n",
    "\n",
    "Exemplo de uso:\n",
    "\n",
    "```py\n",
    "df.filter(\"idade > 18\")\n",
    "df.filter(col(\"nome\").startswith(\"A\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3a8d633-a7a7-4d06-877b-1bcc194e1502",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761162823576}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Vizualizando os dados sem salvar em um DF novo ou sobrescrever o antigo \n",
    "display(df.filter(df.UF == \"MG\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7086451-e104-4335-91e3-51bcbe0f73f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Salvando em um novo DF\n",
    "df_MG = df.filter(df.UF == \"MG\")\n",
    "display(df_MG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc12fb88-e7fd-4aba-bab9-3d8d48ba2500",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Operadores lógicos\n",
    "\n",
    "* **&** = \"E\" lógico,  tem que cumprir todos os criterios para retornar o resultado 1° filtro e o 2° filtro o que tiver resultado igual nos 2 criterios ele vai trazer \n",
    "\n",
    "* **|** = \"OU\" lógico, quando obedecer um criterio ou o outro ele vai trazer \n",
    "\n",
    "* **!=** = NÃO lógico, quando o critério for o oposto do inserido.\n",
    "\n",
    "Nomes:\n",
    "\n",
    "* **&** = e comercial\n",
    "* **|** = \"barra vertical\" ou \"pipe\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32b9be9d-b789-4e61-badd-07001378f870",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Passar cada Criterio emtre parenteses com o operador lógico no meio para separar os criterios \n",
    "display(df.filter((df.UF == \"MG\") & (df.Classificacao_da_Ocorrência == \"Incidente\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5775819b-9b33-488f-925b-528bb354bb2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.filter((df.Danos_a_Aeronave == \"leve\") | (df.Classificacao_da_Ocorrência == \"Incidente\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30ed1a23-a670-4f15-98fd-48a89b33fe0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.filter((df.Danos_a_Aeronave == \"Leve\") & (df.Classificacao_da_Ocorrência == \"Incidente\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "410cef2d-343e-4662-8a90-caf026103b16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Salvando em um Df novo\n",
    "dfnovo = df.filter((df.Danos_a_Aeronave != \"Leve\") & (df.Classificacao_da_Ocorrência == \"Incidente\"))\n",
    "display(dfnovo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59a1d56b-b8df-4945-8179-03a30e32552a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Metodo where\n",
    "\n",
    "O método `.where()` do PySpark DataFrame é utilizado para filtrar linhas com base em condições específicas, funcionando de maneira equivalente ao método `.filter()`. Ele permite selecionar apenas os dados que atendem a determinados critérios, facilitando a análise e manipulação de grandes volumes de dados.\n",
    "\n",
    "**Principais parâmetros do `.where`:**\n",
    "\n",
    "- **condition**: Expressão booleana que define o critério de filtragem. Pode ser uma string SQL (ex: `\"coluna = 'valor'\"`) ou uma expressão construída com funções do PySpark (ex: `col(\"coluna\") > 10`).\n",
    "\n",
    "- **subset** *(opcional, em alguns contextos)*: Lista de colunas a serem consideradas na filtragem, geralmente usada em funções auxiliares.\n",
    "\n",
    "Exemplo de uso:\n",
    "\n",
    "```py\n",
    "df.where(\"idade >= 21\")\n",
    "df.where(col(\"nome\").contains(\"Silva\"))\n",
    "```\n",
    "\n",
    "#### Diferença entre `.filter()` e `.where()`\n",
    "\n",
    "No PySpark, os métodos `.filter()` e `.where()` são funcionalmente equivalentes: ambos servem para filtrar linhas de um DataFrame com base em uma condição booleana. Não há diferença de desempenho ou sintaxe entre eles, sendo a escolha apenas uma questão de preferência ou legibilidade do código.\n",
    "\n",
    "Exemplo equivalente:\n",
    "\n",
    "```py\n",
    "df.filter(df[\"idade\"] > 18)\n",
    "df.where(df[\"idade\"] > 18)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac860868-0051-4423-ada1-9446b006e52e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualizando os dados nao estou salvando em lugar nenhum \n",
    "display(df.where(df.UF == 'SP'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c827aed8-d064-4f95-9133-21b32c9b3bd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.where((df.Danos_a_Aeronave == \"Leve\") & (df.Classificacao_da_Ocorrência == \"Incidente\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fedc057c-3689-4d2c-a3a1-7e1d9431e4d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Salvando em um DF\n",
    "DfFiltrado = df.where((df.Danos_a_Aeronave == \"Leve\") & (df.Classificacao_da_Ocorrência == \"Incidente\"))\n",
    "display(DfFiltrado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ef3792a-cdf8-4389-8dc9-965d325949a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Método Comando SQL\n",
    "\n",
    "Documentação https://spark.apache.org/docs/latest/api/sql/\n",
    "\n",
    "O comando `.sql` no Databricks permite executar consultas SQL diretamente em notebooks, facilitando a análise e manipulação de dados usando a linguagem SQL padrão. Esse método é útil para quem já está familiarizado com SQL e deseja realizar operações como seleção, filtragem, agregação e transformação de dados de forma rápida e intuitiva.\n",
    "\n",
    "**Principais parâmetros do `.sql`:**\n",
    "\n",
    "- **query**: (obrigatório) String contendo a consulta SQL a ser executada. Exemplo: `\"SELECT * FROM tabela WHERE coluna = 'valor'\"`.\n",
    "\n",
    "- **args**: (opcional) Dicionário de parâmetros para interpolação na query, permitindo consultas dinâmicas.\n",
    "\n",
    "- **database**: (opcional) Nome do banco de dados a ser utilizado como contexto padrão para a consulta.\n",
    "\n",
    "- **options**: (opcional) Dicionário com opções adicionais de execução, como configurações de timeout ou limites de resultados.\n",
    "\n",
    "Exemplo de uso:\n",
    "```py\n",
    "spark.sql(\"SELECT nome, idade FROM pessoas WHERE idade > 18\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14953403-e7fa-4b7c-b8e1-e0526f084694",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Criar uma tabela temporaria\n",
    "df.createOrReplaceTempView(\"temp_teste\")\n",
    "mg = spark.sql('''\n",
    "SELECT \n",
    "  `Classificacao_da_Ocorrência` as Classificacao,\n",
    "  Descricao_do_Tipo             as Tipo,\n",
    "  Fase_da_Operacao              as Fase,\n",
    "  Municipio,\n",
    "  UF                            as Estado\n",
    "FROM temp_teste\n",
    "WHERE UF = \"MG\" \n",
    "and Fase_da_Operacao = \"Decolagem\"\n",
    "''')\n",
    "\n",
    "display(mg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c82d378-a24d-4e7f-9fe0-cf16863c4c75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exemplo sem as 3 aspas simples ou Duplas\n",
    "mg = spark.sql('SELECT * FROM temp_teste WHERE UF = \"MG\"')\n",
    "display(mg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44a27d2d-4e42-4ace-bd70-14785f4c8229",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "show databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "092d2030-be7d-4bda-aeda-a1dd25bf780d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "show tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcbee79f-445c-4325-a200-35e98d869d0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS temp_teste;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2daf4757-1ee9-4601-9037-2adc1051f222",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Functions do Spark\n",
    "\n",
    "As funções do Spark são essenciais para manipulação, transformação e análise de dados em DataFrames e consultas SQL. Elas abrangem desde operações matemáticas, manipulação de strings, datas, arrays, até funções de agregação e criação de funções personalizadas (UDFs). O uso eficiente dessas funções permite realizar análises complexas de forma simples e performática no Databricks e em ambientes Spark em geral.\n",
    "\n",
    "Documentação functions\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-ref-functions.html\n",
    "\n",
    "Documentação functions string\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-ref-functions-builtin.html#string-functions\n",
    "\n",
    "\n",
    "O PySpark fornece uma ampla gama de funções dentro do módulo `pyspark.sql.functions`.\n",
    "Alguns Exemplos\n",
    "\n",
    "| Função | Descrição | Exemplo |\n",
    "|--------|-----------|---------|\n",
    "| **col** | Retorna uma coluna do DataFrame com base no nome fornecido. | `col(\"nome_da_coluna\")` |\n",
    "| **concat** | Concatena várias colunas em uma única coluna. | `concat(col(\"coluna1\"), lit(\" \"), col(\"coluna2\"))` |\n",
    "| **substring** | Retorna uma parte de uma coluna de texto com base em índices. | `substring(col(\"texto\"), 1, 3)` |\n",
    "| **length** | Retorna o comprimento de uma coluna de texto. | `length(col(\"texto\"))` |\n",
    "| **lower** | Converte o texto de uma coluna para minúsculas. | `lower(col(\"texto\"))` |\n",
    "| **upper** | Converte o texto de uma coluna para maiúsculas. | `upper(col(\"texto\"))` |\n",
    "| **trim** | Remove espaços em branco do início e do final de uma coluna de texto. | `trim(col(\"texto\"))` |\n",
    "| **replace** | Substitui um padrão por outro em uma coluna de texto. | `replace(col(\"texto\"), \"old\", \"new\")` |\n",
    "| **regexp_replace** | Substitui um padrão de expressão regular por outro em uma coluna de texto. | `regexp_replace(col(\"texto\"), \"pattern\", \"replacement\")` |\n",
    "| **split** | Divide uma coluna de texto em um array com base em um delimitador. | `split(col(\"texto\"), \" \")` |\n",
    "| **substring_index** | Retorna as primeiras n ocorrências de um delimitador em uma coluna de texto. | `substring_index(col(\"texto\"), \" \", 2)` |\n",
    "| **concat_ws** | Concatena colunas usando um delimitador especificado. | `concat_ws(\" \", col(\"coluna1\"), col(\"coluna2\"))` |\n",
    "| **when** | Realiza uma operação condicional em uma coluna com base em uma condição. | `when(col(\"idade\") >= 18, \"adulto\").otherwise(\"menor\")` |\n",
    "| **coalesce** | Retorna a primeira coluna não nula de um conjunto de colunas. | `coalesce(col(\"coluna1\"), col(\"coluna2\"), col(\"coluna3\"))` |\n",
    "\n",
    "Essas são apenas algumas das muitas funções disponíveis no módulo `pyspark.sql.functions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04f89817-c386-4819-93d2-7a6d90064e73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "791d7910-c0fe-4e1d-b777-c600c1cee626",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Retorna uma coluna do DataFrame passando nome como referencia\n",
    "display(df.filter(col('UF') == 'SP'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "395b0964-c68e-4f17-a65b-5b2f695d1418",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Filtrando pelo indice da coluna\n",
    "\n",
    "Em algumas situações, pode ser necessário filtrar dados de um DataFrame utilizando o índice (posição) das colunas, especialmente quando não se conhece previamente o nome das colunas ou quando se deseja aplicar filtros de forma dinâmica. No PySpark, é possível acessar colunas por índice utilizando a lista de nomes de colunas do DataFrame, permitindo construir filtros flexíveis e adaptáveis a diferentes estruturas de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2043b32-b8b5-451c-95aa-5b9f28ffc9b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "colunas = df.columns\n",
    "display(colunas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ad27753-a27a-4a2f-81d6-3059c789a196",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Localizando Indice em Dados com varias colunas \n",
    "Coluna = \"Operador\"\n",
    "Indice = df.columns.index(Coluna)\n",
    "display(Indice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d8b8b55-1232-4c85-8d97-3ddcebbd201f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Gera um dataframe com a lista de colunas e seus indices\n",
    "ListaIndices = []\n",
    "\n",
    "# Loop \n",
    "for Coluna in df.columns:\n",
    "    Indice = df.columns.index(Coluna)\n",
    "    ListaIndices.append((Coluna, Indice))\n",
    "\n",
    "# Salvar lista em DF\n",
    "df_indices = spark.createDataFrame(ListaIndices, [\"Coluna\", \"Indice\"])\n",
    "\n",
    "display(df_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19f34516-368c-4d03-b312-e0f6b8a7d480",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filtrando pelo indice \n",
    "df_filtrado = df.filter(col(df.columns[43]) == \"SCF-PP\") \n",
    "display(df_filtrado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d4b5adb-d320-4885-b477-c2d98ee9babb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inserindo coluna e filtro por variaveis\n",
    "Coluna1 = 'Fase_da_Operacao'\n",
    "Filtro1 = 'Decolagem'\n",
    "Coluna2 = 'UF'\n",
    "Filtro2 = 'MG'\n",
    "\n",
    "display(df.filter( (df[Coluna1] == Filtro1) & (df[Coluna2] == Filtro2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b38c7bb6-bf3f-44be-aa1a-f4801ec77813",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Corrigindo erro de maiúsculas e minúsculas\n",
    "\n",
    "Ao trabalhar com dados textuais em DataFrames, é comum encontrar inconsistências relacionadas ao uso de letras maiúsculas e minúsculas, o que pode impactar buscas, comparações e filtragens. Para garantir resultados precisos e evitar erros, é importante padronizar o formato dos textos, convertendo-os para caixa alta (maiúsculas) ou caixa baixa (minúsculas) antes de realizar operações de análise. O PySpark oferece funções específicas para facilitar essa padronização de forma eficiente.\n",
    "\n",
    "* **lower**: Converte o texto de uma coluna para minúsculas.\n",
    "* **upper**: Converte o texto de uma coluna para maiúsculas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72adb0f0-c21a-4dd4-b48a-9e26b64e0c5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.filter(df.Fase_da_Operacao == 'Decolagem'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b76c5344-7107-4b86-9e8c-b314dd6405dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.filter(df.Fase_da_Operacao == 'decolagem'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08df9e92-a03f-4c72-8237-6b47f2cd2d1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import upper, lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9573d9d9-fa49-4bca-a29d-22d252c39545",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Correção para bases grandes\n",
    "# 1° verificar comportamento \n",
    "# 2° criar um padrão para as colunas\n",
    "\n",
    "display(df.select(\"Fase_da_Operacao\").distinct())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab4bc084-654c-4471-b187-a004f81223c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Upper  = Transforma dados da coluna em maiúsculas\n",
    "display(df.select(upper(\"Fase_da_Operacao\")).distinct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc3b81d3-e44d-4a95-b4be-4c512a2b83d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# lower  = Transforma dados da coluna em minúsculas \n",
    "display(df.select(lower(\"Fase_da_Operacao\")).distinct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a69fc65-a6ff-48b8-a63b-7d049f7729e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filtro sem erro eu resolvi fazer minúsculas\n",
    "display(df.filter(lower(df.Fase_da_Operacao) == 'decolagem'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bd7957e-2391-427e-8687-c7832bc51a44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Notem que no dataframe Permanece como Maiusculo, usei somente no filtro para nao haver erro. O DF permanece original depois vamos aprender a alterar direto na base \n",
    "novo_df = df.filter(lower(df.Fase_da_Operacao) == 'decolagem')\n",
    "display(novo_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0630a830-a835-4d20-bc9d-d87a268f9f40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Resolvendo erros de Digitação com Espaços Excessivos\n",
    "\n",
    "Ao lidar com dados textuais, é comum encontrar registros com espaços em branco indesejados no início, no final ou até mesmo entre as palavras. Esses espaços podem causar problemas em buscas, comparações e análises, levando a resultados incorretos ou inconsistentes. Nesta seção, vamos abordar técnicas para identificar e remover espaços excessivos em colunas de texto utilizando funções do PySpark, garantindo maior precisão e padronização nos dados.\n",
    "\n",
    "* **trim**: Remove espaços em branco do início e do final de uma coluna de texto.\n",
    "* **rtrim**: Remove espaços em branco da direita \n",
    "* **ltrim**: Remove espaços em branco da esquerda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d497374-0c75-4624-bfde-fb1ccd2c89df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Dados = [(\"Belo Horizonte \", \"MG\", 2512070),\n",
    "        (\" Belo Horizonte \", \"MG\", 12106920),\n",
    "        (\"Rio de Janeiro\", \"RJ\", 6718903),\n",
    "        (\" Belo Horizonte\", \"MG\", 20001151),\n",
    "        (\"Brasília\", \"DF\", 3015268)]\n",
    "\n",
    "Colunas = [\"Cidade\", \"UF\", \"População\"]\n",
    "\n",
    "new_df = spark.createDataFrame(Dados, Colunas)\n",
    "display(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d59a3404-59ce-4a0e-8c24-eae425ab8c00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(new_df.filter(new_df.Cidade == 'Belo Horizonte'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "860c827a-29d9-4655-a914-eb5a4cd1bde9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rtrim, ltrim, trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17e234c0-8c0f-4b54-bed6-f219f6f87cbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Neste ele eliminou o espaço da esquerda e trouxe o resultado\n",
    "display(new_df.filter(ltrim(new_df.Cidade) == 'Belo Horizonte'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69185cbc-1b05-4dcf-aa08-e5a1fc87c427",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Neste ele eliminou o espaço da direita e trouxe o resultado\n",
    "display(new_df.filter(rtrim(new_df.Cidade) == 'Belo Horizonte'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ec59329-334b-4575-b773-82195ac6e7b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Neste estamos usando os 2 métodos para eliminar espaço na esquerda e direita\n",
    "display(new_df.filter(rtrim(ltrim(new_df.Cidade)) == 'Belo Horizonte'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3073ecf4-20c2-4f24-b7ca-31167e9d402a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ja elimina os 2 o melhor de usar é esse\n",
    "display(new_df.filter(trim(new_df.Cidade) == 'Belo Horizonte'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef7c6c15-c449-4a89-8b99-05570eb69a89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Arrumando o DF com o withColumn que adiciona ou substitui uma coluna (mesmo nome substitui nome diferente adiciona)\n",
    "new_df = new_df.withColumn(\"Cidade\", trim(new_df.Cidade))\n",
    "display(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03b98c91-ec5e-4cfa-ae95-7038e347aa50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filtro sem usar o trim ja foi tratado antes \n",
    "display(new_df.filter(new_df.Cidade == 'Belo Horizonte'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a4df3a1-dc9a-47a4-a431-342d3616f890",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Substituindo Dados avançados \n",
    "\n",
    "Substituir dados em um DataFrame é uma tarefa comum durante o processo de limpeza e padronização de informações. No Databricks, é possível realizar substituições avançadas utilizando funções do PySpark, como `replace`, `regexp_replace` e outras, permitindo corrigir valores inconsistentes, padronizar formatos ou tratar erros de digitação de forma eficiente. Nesta seção, vamos explorar técnicas para substituir dados de maneira flexível, tanto em colunas específicas quanto em múltiplas colunas, utilizando expressões regulares e condições personalizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a4e45e0-f43a-4392-9259-c856d45365be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simulando dados para os 27 estados do Brasil\n",
    "dados = [\n",
    "    (\"Acre\", \"AC\", 894470),\n",
    "    (\"Alagoas\", \"AL\", 3337357),\n",
    "    (\"Amapá\", \"AP\", 861773),\n",
    "    (\"Amazonas\", \"AM\", 4207714),\n",
    "    (\"Bahia\", \"BA\", 14930634),\n",
    "    (\"Ceará\", \"CE\", 9132078),\n",
    "    (\"Distrito Federal\", \"DF\", 3055149),\n",
    "    (\"Espírito Santo\", \"ES\", 4018650),\n",
    "    (\"Goiás\", \"GO\", 7113540),\n",
    "    (\"Maranhão\", \"MA\", 7114598),\n",
    "    (\"Mato Grosso\", \"MT\", 3526220),\n",
    "    (\"Mato Grosso do Sul\", \"MS\", 2809394),\n",
    "    (\"Minas Gerais\", \"MG\", 21168791),\n",
    "    (\"Pará\", \"PA\", 8690745),\n",
    "    (\"Paraíba\", \"PB\", 4039277),\n",
    "    (\"Paraná\", \"PR\", 11433957),\n",
    "    (\"Pernambuco\", \"PE\", 9616621),\n",
    "    (\"Piauí\", \"PI\", 3273227),\n",
    "    (\"Rio de Janeiro\", \"RJ\", 17366189),\n",
    "    (\"Rio Grande do Norte\", \"RN\", 3534165),\n",
    "    (\"Rio Grande do Sul\", \"RS\", 11422973),\n",
    "    (\"Rondônia\", \"RO\", 1796460),\n",
    "    (\"Roraima\", \"RR\", 631181),\n",
    "    (\"Santa Catarina\", \"SC\", 7252502),\n",
    "    (\"São Paulo\", \"SP\", 46289333),\n",
    "    (\"Sergipe\", \"SE\", 2318822),\n",
    "    (\"Tocantins\", \"TO\", 1590248),\n",
    "]\n",
    "\n",
    "colunas = [\"Estado\", \"UF\", \"População\"]\n",
    "\n",
    "# Criando o DataFrame\n",
    "df_estados = spark.createDataFrame(dados,colunas)\n",
    "display(df_estados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79ac5892-3e7b-43f8-ad7b-2623f9a50f31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc1d05cd-1338-41b2-a796-a0a2947d7caa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Poderia localidar a coluna tambm df_estados[\"Estado\"] ou pelo indice\n",
    "df_estados1 = df_estados.withColumn(\"EstadoSemAcento\", regexp_replace(df_estados.Estado, \"á\", \"a\"))\n",
    "\n",
    "display(df_estados1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73b68dc0-5a3a-4362-b4a6-c4396829d00d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import translate\n",
    "# tem que ser na mesma ordem \n",
    "acentos     = \"áàãâéèêíìóòõôúùûç\"\n",
    "sem_acentos = \"aaaaeeeiioooouuuc\"\n",
    "\n",
    "# Aplicar a substituição usando a função translate\n",
    "df_estados2 = df_estados.withColumn(\"EstadoSemAcento\", translate(df_estados.Estado, acentos, sem_acentos))\n",
    "\n",
    "# Exibir o DataFrame\n",
    "display(df_estados2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d985e9ef-be07-48f6-8a1b-cb4782a0f3c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Usando translatee lower pra nao ter problema de forma alguma tudo sem acento e minusculo\n",
    "from pyspark.sql.functions import translate,lower\n",
    "acentos     = \"áàãâéèêíìóòõôúùûç\"\n",
    "sem_acentos = \"aaaaeeeiioooouuuc\"\n",
    "\n",
    "df_estados3 = df_estados.withColumn(\"EstadoSemAcento\", lower(translate(df_estados.Estado, acentos, sem_acentos)))\n",
    "display(df_estados3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c7c4519-dec5-45e3-aef0-bf415f98c128",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_estados3.filter(df_estados3.EstadoSemAcento == 'Maranhao' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60a227bf-0aa5-4a9d-a379-ceccf228dfb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Usando todas as funções possiveis: sem acento, tudo minusculo e sem espaços escessivos\n",
    "from pyspark.sql.functions import translate,lower,trim\n",
    "acentos     = \"áàãâéèêíìóòõôúùûç\"\n",
    "sem_acentos = \"aaaaeeeiioooouuuc\"\n",
    "\n",
    "df_estados4 = df_estados.withColumn(\"EstadoSemAcento\", trim(lower(translate(df_estados.Estado, acentos, sem_acentos))) )\n",
    "display(df_estados4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "910829b7-435e-45db-a538-c2e5fe8ac463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_estados4.filter(df_estados4.EstadoSemAcento == 'sao paulo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37903bbd-eca6-4daa-b31d-da934b0d0703",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760468887091}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Subtituindo coluna e nao adicionando uma nova \n",
    "from pyspark.sql.functions import translate,lower,trim\n",
    "acentos     = \"áàãâéèêíìóòõôúùûç\"\n",
    "sem_acentos = \"aaaaeeeiioooouuuc\"\n",
    "\n",
    "df_estados = df_estados.withColumn(\"Estado\", trim(lower(translate(df_estados.Estado, acentos, sem_acentos))))\n",
    "display(df_estados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14090b6e-fd7c-4adf-b8b7-3f9ec5da97c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Função Like\n",
    "\n",
    "A função `like` é utilizada em consultas SQL e em métodos do PySpark para filtrar linhas de um DataFrame com base em padrões de texto. Ela permite buscar valores que correspondam parcialmente a um critério, utilizando curingas como `%` (qualquer sequência de caracteres) e `_` (um único caractere). Essa função é especialmente útil para localizar registros que contenham, comecem ou terminem com determinados trechos de texto, facilitando buscas flexíveis e análises exploratórias em grandes volumes de dados.\n",
    "\n",
    "* **%Texto%** = traz todo o resto pra frente e pra tras do texto\n",
    "* **Texto%** = inicia com o texto e nao importa o que tem pra frente\n",
    "* **%Texto** = finaliza com o texto nao importa o que tem para tras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "656ffc9e-2b26-4d0a-a227-066f649d22f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Passar filtro por trecho especifico % a grosso modo significa todo o resto, voce vai entender na prática\n",
    "resultado = df.filter(df.Descricao_do_Tipo.like(\"%AVE\"))\n",
    "display(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfe1a87e-b009-49ae-bdff-d9897456097e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Filtro de Valores\n",
    "\n",
    "Filtrar valores em um DataFrame é uma etapa essencial para refinar a análise de dados, permitindo selecionar apenas os registros que atendem a critérios específicos. No Databricks, é possível aplicar filtros de diversas formas, seja utilizando métodos do PySpark, comandos SQL ou até mesmo recursos interativos na interface. Nesta seção, vamos apresentar as principais técnicas para filtrar valores de maneira eficiente, facilitando a obtenção de insights relevantes a partir dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "503edd76-fdc1-4360-83ff-cbad9fb69950",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orders_df = spark.read. \\\n",
    "    format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"/Volumes/workspace/default/tutorial/assets/orders.csv\")\n",
    "\n",
    "display(orders_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c565e155-1014-48e9-b986-487d62c7c9cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(orders_df.filter(orders_df.order_status == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18166f77-d85a-4ce3-b26c-b02bc2515058",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Salvando em um df simulando que 1 seja pedidos pendentes \n",
    "PedidosPendentes = orders_df.filter(orders_df.order_status == 1) \n",
    "display(PedidosPendentes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "221bcabc-a59b-4b38-91a4-392db756744a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pedidos de uma loja especifica \n",
    "Loja2 = orders_df.filter(orders_df.store_id == 2) \n",
    "display(Loja2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34e839b3-b35b-4173-a046-1f355a8d4a86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pedidos diferentes de uma loja espefica\n",
    "fora3 = orders_df.filter(orders_df.store_id != 3) \n",
    "display(fora3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a41686a8-48f8-496d-91d7-c3f60281ad3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filtro composto exemplo pedidos da Loja 3 com status =1\n",
    "StatusLoja3 = orders_df.filter((orders_df.store_id == 3) & (orders_df.order_status == 1))  \n",
    "display(StatusLoja3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d4b82f6-3a11-421a-80e8-46ffcd9f7d97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "order_items = spark.read. \\\n",
    "    format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"/Volumes/workspace/default/tutorial/assets/order_items.csv\")\n",
    "\n",
    "display(order_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45e619c0-9a65-4465-afe6-89483fe41d6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exemplos de faixa de preços\n",
    "Barato = order_items.filter(order_items.list_price < 300 ) \n",
    "display(Barato)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0b3ac97-17a4-4a35-8f40-00d820619e1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Caro = order_items.filter(order_items.list_price > 2000 ) \n",
    "display(Caro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a981162-f875-41e4-96ba-c2ab318f7ac5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "medio = order_items.filter((order_items.list_price >= 700) & (order_items.list_price <= 1200))\n",
    "display(medio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa87d9eb-2586-40be-ae53-7535c2d0d6bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "medio = order_items.filter((col(\"list_price\") >= 700) & (col(\"list_price\") <= 1200))\n",
    "display(medio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f4f4b88-ae3b-40ca-b3aa-628d04cef4e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ValorMinimo= 50\n",
    "ValorMaximo= 100\n",
    "\n",
    "df_resultado = order_items.filter((order_items.list_price >= ValorMinimo) & (order_items.list_price <= ValorMaximo))\n",
    "display(df_resultado)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8415805086659366,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "11 - Filtro de Dados no Databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
