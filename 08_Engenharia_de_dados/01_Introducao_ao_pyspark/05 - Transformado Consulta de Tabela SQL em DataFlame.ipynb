{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6de5a08-bf40-4dad-9276-5802ddbde014",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Consultando as tabelas criadas\n",
    "\n",
    "Nesta seção, vamos aprender como consultar as tabelas que foram criadas em nosso ambiente Databricks. Entender como acessar e visualizar os dados armazenados é fundamental para realizar análises, validar resultados e dar continuidade ao processamento de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34ac938e-bab8-40c1-b445-cd2e6eeccb9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from bike_store.clients;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a162e28-8b34-47e3-ae80-70e9f128ebb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--  Consulta de Contatos de Clientes\n",
    "select \n",
    "  customer_id\n",
    "  ,first_name\n",
    "  ,phone\n",
    "  ,email\n",
    "from bike_store.clients; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d1cbf6a-265c-4f57-98c3-4b89f94abd25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--  Consulta de Contatos de Clientes tem contém sha no primeiro nome\n",
    "select \n",
    "    customer_id\n",
    "    ,first_name\n",
    "    ,phone\n",
    "    ,email\n",
    "from bike_store.clients\n",
    "where first_name like \"%sha%\";\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5a4e9fa-39e2-4a18-9b1d-a6369ab93d58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Transformando consulta SQL em dataframes\n",
    "\n",
    "Transformar consultas SQL em DataFrames é uma prática comum no Databricks, permitindo combinar a flexibilidade da linguagem SQL com o poder de processamento dos DataFrames do Spark. Dessa forma, é possível realizar análises avançadas, manipular dados de maneira eficiente e integrar diferentes etapas do pipeline de dados utilizando tanto comandos SQL quanto operações em DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f26e9f2d-02c6-4c3a-bbf7-7304c7fd46b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df =  spark.sql('''\n",
    "    select\n",
    "        customer_id as id\n",
    "        ,first_name as name\n",
    "        ,phone as phone\n",
    "        ,email as email\n",
    "    from bike_store.clients\n",
    "    where first_name like \"%sha%\";         \n",
    "'''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc367b9e-0f66-49c0-ae5e-9f03c245a8ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d795131c-734b-4b92-b465-7926e63108f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"select * from bike_store.clients\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5636474d-99fe-44aa-ae16-eff6f0d6233e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Salvando o dataframe em diferentes formatos\n",
    "\n",
    "Salvar DataFrames em diferentes formatos é uma etapa essencial no processamento de dados, pois permite compartilhar, armazenar e reutilizar informações de maneira eficiente. No Databricks, é possível exportar DataFrames para diversos formatos populares, como CSV, Parquet, Delta e outros, facilitando a integração com diferentes sistemas e fluxos de trabalho. Nesta seção, veremos como realizar esse procedimento de forma prática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98ef6c97-692e-4cab-927f-3c091d173cc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df =  spark.sql('''\n",
    "    select \n",
    "        customer_id as id\n",
    "        ,first_name as name\n",
    "        ,phone as phone\n",
    "        ,email as email\n",
    "    from bike_store.clients\n",
    "    where first_name like \"%sha%\"\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1600ea67-1524-4ddf-9655-8b714df83956",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d4f471e-22c1-4bd8-9ab5-2fcb4857281c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls('/Volumes/workspace/bike_store/outputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e13f9f76-fac2-4f00-ad46-f492a571ad7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Salvando no formato Parquet\n",
    "\n",
    "O formato Parquet é amplamente utilizado em ambientes de Big Data por ser eficiente no armazenamento e leitura de grandes volumes de dados. Ele organiza as informações de forma colunar, o que permite compressão otimizada e consultas mais rápidas, especialmente em operações analíticas. Nesta seção, veremos como salvar DataFrames no formato Parquet utilizando o Databricks, garantindo portabilidade e performance no processamento dos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c0481b8-c6a3-4d1e-9cf2-3c39ea35dde1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Termos Tecnicos Para voce ir treinando \n",
    "* **write**:  Escrever/ Gravar\n",
    "\n",
    "**Modos de Escritas Mais usados**:\n",
    "\n",
    "* **overwrite**   = se existe ele sobescreve/substitui\n",
    "* **append**      = Mantem o existente e adiciona o conteudo no final \n",
    "* **ignore**      = Usado para salvar os dados apenas se o local de destino não existir\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6895b09b-14fc-4e19-97cf-a00c688e6ce2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"/Volumes/workspace/bike_store/outputs/contact_sha.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6055e492-51c8-494d-8961-72a76f28664e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls('/Volumes/workspace/bike_store/outputs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5c139b7-b0cf-4847-8b24-1b94e75e662b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls('/Volumes/workspace/bike_store/outputs/contact_sha.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ceafb0b-4235-4819-81ca-a0de150b714b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Salvar em CSV e JSON\n",
    "\n",
    "Salvar DataFrames em diferentes formatos, como CSV e JSON, é uma prática fundamental para garantir a portabilidade e o compartilhamento dos dados processados. Esses formatos são amplamente utilizados em integrações com outros sistemas, análises externas e armazenamento de dados intermediários. A seguir, veremos como exportar DataFrames para CSV e JSON no Databricks de forma simples e eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe144f6a-8d95-4e74-ab7e-4183bac8c72d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Salvar em CSV e JSON\n",
    "df.write \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(\"/Volumes/workspace/bike_store/outputs/contact_sha.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e8b0bef-3f29-4851-88ce-0fc3cf4ef059",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .json(\"/Volumes/workspace/bike_store/outputs/contact_sha.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f9b51ae-c166-4a88-8606-1dba5da89aa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls('/Volumes/workspace/bike_store/outputs'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bca3351d-5562-45f1-95ff-a1ee29fa9a1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ler arquivos CSV , Json e Parquet\n",
    "\n",
    "Ler arquivos em diferentes formatos é uma etapa essencial no processamento de dados, pois permite integrar informações provenientes de diversas fontes e sistemas. No Databricks, é possível importar dados em formatos como CSV, JSON e Parquet de maneira simples e eficiente, facilitando a análise e o tratamento dos dados. Nesta seção, veremos como realizar a leitura desses arquivos e transformar seu conteúdo em DataFrames para posterior manipulação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f16fa243-3eaa-42d8-b08f-4d946be87dd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = \"/Volumes/workspace/bike_store/outputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc73da6f-a0fe-4f83-969e-8799258869c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ler o arquivo CSV em um DataFrame\n",
    "df_csv = spark.read.csv(f\"{path}/contact_sha.csv\", header=False, inferSchema=True)\n",
    "display(df_csv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ef5169b-f89b-4f8f-8a6c-df7f9b5beb24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ler o arquivo JSON em um DataFrame\n",
    "df_json = spark.read.json(f\"{path}/contact_sha.json\")\n",
    "display(df_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc103817-fb35-4122-8f78-0bacd8ecdfde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ler o arquivo Sem Salvar em Um DF\n",
    "df_parquet =  spark.read.parquet(f\"{path}/contact_sha.parquet\")\n",
    "display(df_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7300286-4049-45fd-874d-9a160bbfffdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Gravação e Leitura particionado\n",
    "\n",
    "A gravação e leitura de dados particionados é uma técnica fundamental para otimizar o desempenho e a organização dos dados em ambientes de Big Data, como o Databricks. Ao particionar arquivos, é possível dividir grandes volumes de dados em subconjuntos menores com base em valores de uma ou mais colunas, facilitando consultas mais rápidas e eficientes. Nesta seção, veremos como salvar e ler arquivos particionados, destacando as vantagens dessa abordagem para o processamento e análise de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e76c60f3-b2a2-459f-88e4-f09db858b2c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from bike_store.clients;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07390e2d-9541-4627-b8d4-34394dda7c24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "novodf = spark.sql('select * from bike_store.clients')\n",
    "display(novodf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20f259b3-eea6-4015-8c7c-76e03d824212",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Gravar Particionado\n",
    "novodf.write\\\n",
    "    .partitionBy(\"state\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet('/Volumes/workspace/bike_store/outputs/contact_sha.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0cd554a-335b-4820-916f-d93a95453e3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls('/Volumes/workspace/bike_store/outputs/contact_sha.parquet/state=CA/'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d0dbed6-c31f-4030-b38e-e1f507a35541",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ler Arquivo particionado \n",
    "state_CA = spark.read.parquet('/Volumes/workspace/bike_store/outputs/contact_sha.parquet/state=CA/')\n",
    "display(state_CA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30dadb5b-b04f-442e-8422-a3db970e1ff0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls('dbfs:/FileStore/tables/DiferentesSaidas/Contato_Particionado.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfe395bf-8f35-4f67-92df-3fd0b362e9a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Ler Arquivo particionado (Geral) \n",
    "state_CA = spark.read.parquet('dbfs:/FileStore/tables/DiferentesSaidas/Contato_Particionado.parquet')\n",
    "display(state_CA)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7058022900491098,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "05 - Transformado Consulta de Tabela SQL em DataFlame",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
