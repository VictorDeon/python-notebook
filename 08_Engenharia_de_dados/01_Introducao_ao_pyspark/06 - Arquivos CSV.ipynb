{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4225feea-5f60-41b3-8afc-a67f4d8880e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Documentação CSV\n",
    "\n",
    "O formato CSV (Comma-Separated Values) é amplamente utilizado para armazenamento e intercâmbio de dados tabulares devido à sua simplicidade e compatibilidade com diversas ferramentas e plataformas. Nesta seção, exploraremos a documentação e os principais recursos para manipulação de arquivos CSV, especialmente no contexto do Databricks, facilitando a leitura, escrita e processamento eficiente desses dados em ambientes de análise e ciência de dados.\n",
    "\n",
    "### Dados abertos\n",
    "\n",
    "Pesquisar google dados gov dados publicos\n",
    "\n",
    "https://dados.gov.br/dados/conjuntos-dados\n",
    "\n",
    "Defesa e segurança Formato csv > Recursos\n",
    "\n",
    "https://dados.gov.br/dados/conjuntos-dados/ocorrencias-aeronauticas\n",
    "\n",
    "### Pesquisar csv documentation databricks\n",
    "\n",
    "https://docs.databricks.com/pt/external-data/csv.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-data-sources-csv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b5c9a55-4216-4c71-8a15-7cc18324127e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = \"/Volumes/workspace/default/tutorial/anac\"\n",
    "display(dbutils.fs.ls(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4c2be02-42e2-4d04-8314-d8bfc438bf60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ler o arquivo CSV em um DataFrame  modo antigo sem tratamento\n",
    "df_csv = spark.read.csv(f\"{path}/V_OCORRENCIA_AMPLA.csv\")\n",
    "display(df_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2fd7f9f-f322-4c18-a3b1-33a5851e51d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Passando Opções avançadas\n",
    "# Modo de Leitura do spark\n",
    "# .format: Formato do Arquivo\n",
    "# skipRows: Quantas Linhas Pular\n",
    "# header: 1° linha é o Cabeçalho?\n",
    "# sep: Separador do CSV\n",
    "# inferSchema: Inserir os tipos de Dados automaticamente (se não colocar vai vir tudo string)\n",
    "df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"skipRows\", 1) \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"sep\", \";\") \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .load(f\"{path}/V_OCORRENCIA_AMPLA.csv\")\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62c5e38c-fed9-4374-9db0-c69ad01aa529",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ajustando Colunas do CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04486952-3ccf-47d8-89bd-64b3df91ef22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CSV ContatoShar da aula anterior (3. Transformado Consulta de Tabela SQL em DataFlame)\n",
    "caminho_csv = \"/Volumes/workspace/bike_store/outputs/contact_sha.csv\"\n",
    "df_csv = spark.read.csv(caminho_csv, header=False, inferSchema=True)\n",
    "display(df_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8738af18-89e1-42bf-a5d2-53d03096b11f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760450961721}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Renomenado colunas\n",
    "df_csv = df_csv.withColumnRenamed(\"_c0\", \"id\") \\\n",
    "               .withColumnRenamed(\"_c1\", \"name\") \\\n",
    "               .withColumnRenamed(\"_c2\", \"phone\") \\\n",
    "               .withColumnRenamed(\"_c3\", \"email\")\n",
    "\n",
    "display(df_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe3acd93-4106-4796-9520-9c4dbfd870de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Salvando Arquivo em Tamanho reduzido\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-data-sources-csv.html\n",
    "\n",
    "Ao lidar com grandes volumes de dados em formato CSV, é comum a necessidade de reduzir o tamanho dos arquivos para otimizar o armazenamento e facilitar o processamento. Nesta seção, abordaremos estratégias e boas práticas para salvar arquivos CSV em tamanhos menores, mantendo a integridade dos dados e melhorando a eficiência em ambientes de análise, como o Databricks.\n",
    "\n",
    "### Conversor de Bytes para Megabytes (MB)\n",
    "\n",
    "Para converter bytes em megabytes (MB), basta dividir o valor em bytes por 1.048.576 (ou seja, 1024 × 1024):\n",
    "\n",
    "\n",
    "MB = bytes / 1.048.576\n",
    "\n",
    "\n",
    "**Exemplo:**  \n",
    "Se um arquivo possui 5.242.880 bytes:\n",
    "\n",
    "\n",
    "MB = 5.242.880 / 1.048.576 = 5 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b2ade5b-48dd-4945-aeb4-d4f09c374270",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Salvando um DataFlame\n",
    "df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"skipRows\", 1) \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"sep\", \";\") \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .load(f\"{path}/V_OCORRENCIA_AMPLA.csv\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b81bddc-b5d3-45f6-8134-9d72a6939c31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Salvando em formato comprimido (melhor para questoes de armazenamento em nuvem onde paga pelo tanto que armazenar)\n",
    "# write = Gravar\\Escrever \n",
    "# overwrite = Sobrescrever o arquivo\n",
    "df.write \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"compression\", \"gzip\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \";\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"{path}/V_OCORRENCIA_AMPLA_CSV.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e14122aa-20ee-4324-a0c3-a6424849de81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe33b8c7-fc63-4cf8-81b9-0ff565cffb09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# O CSV tem 9MB e o ZIP ficou com 2MB\n",
    "def get_folder_size(path):\n",
    "    files = dbutils.fs.ls(path)\n",
    "    return sum(f.size for f in files if not f.isDir())\n",
    "\n",
    "folder_size = get_folder_size(f\"{path}/V_OCORRENCIA_AMPLA.csv\")\n",
    "print(f\"Tamanho total da pasta CSV: {folder_size} bytes\")\n",
    "\n",
    "folder_size = get_folder_size(f\"{path}/V_OCORRENCIA_AMPLA_CSV.zip\")\n",
    "print(f\"Tamanho total da pasta ZIP: {folder_size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b3c1e07-018c-4338-845f-70336bf7e9b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Lendo Arquivo salvo  em tamanho reduzido\n",
    "\n",
    "Ao trabalhar com grandes volumes de dados, é fundamental garantir que os arquivos estejam otimizados para leitura e processamento. Após reduzir o tamanho dos arquivos CSV, o próximo passo é realizar a leitura desses arquivos de forma eficiente, mantendo a integridade e a qualidade das informações. Nesta seção, apresentaremos as principais práticas e comandos para ler arquivos CSV compactados ou otimizados no Databricks, facilitando a análise e o tratamento dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cefcd61d-fd89-4a85-90e3-b6f0c016be70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read = ler fazer a leitura do arquivo\n",
    "dfnovo = spark.read \\\n",
    "            .option(\"compression\", \"gzip\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .option(\"sep\", \";\") \\\n",
    "            .csv(f\"{path}/V_OCORRENCIA_AMPLA_CSV.zip\")\n",
    "\n",
    "display(dfnovo)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06 - Arquivos CSV",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
