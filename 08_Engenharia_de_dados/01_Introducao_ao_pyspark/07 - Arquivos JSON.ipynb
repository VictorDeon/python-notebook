{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c949475d-2411-449d-a9e8-461f89022627",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Ler um arquivo json\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-data-sources-json.html\n",
    "\n",
    "A leitura de arquivos JSON é uma tarefa comum em projetos de análise de dados, pois esse formato é amplamente utilizado para armazenar e transferir informações estruturadas. O Apache Spark oferece suporte nativo para leitura e manipulação de arquivos JSON, permitindo que você carregue, consulte e processe dados de forma eficiente em ambientes distribuídos. Nesta seção, veremos como utilizar o Spark para ler arquivos JSON e explorar seus dados.\n",
    "\n",
    "##### Sites úteis:\n",
    "\n",
    "https://jsonviewer.stack.hu/\n",
    "\n",
    "https://codebeautify.org/jsonviewer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c282ccce-d940-4362-bff8-5e9ef3b38dcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = \"/Volumes/workspace/default/tutorial/anac\"\n",
    "df = spark.read.json(f\"{path}/V_OCORRENCIA_AMPLA.json\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d603a1e2-51d7-4fd7-8bb2-ba6797db84aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Renomear Colunas\n",
    "\n",
    "Renomear colunas é uma etapa fundamental no processo de preparação de dados, pois garante que os nomes dos campos sejam claros, padronizados e adequados ao contexto da análise. No Apache Spark, é possível renomear colunas de forma simples e eficiente, facilitando a manipulação e compreensão dos dados ao longo do pipeline de processamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7cbc6ee-200f-4773-a2a7-6968b082956c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df = df.withColumnRenamed(\"Aerodromo_de_Destino\", \"Destino\") \\\n",
    "    .withColumnRenamed(\"Aerodromo_de_Origem\", \"Origem\") \\\n",
    "    .withColumnRenamed(\"Classificacao_da_Ocorrência\", \"Classificação\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a22ad1d-0d10-455e-8591-13b51946b7b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Salvar arquivo json zipado\n",
    "\n",
    "Salvar arquivos JSON compactados é uma prática comum para otimizar o armazenamento e a transferência de grandes volumes de dados. O Apache Spark permite salvar DataFrames em formato JSON utilizando compressão, como ZIP ou GZIP, reduzindo o espaço ocupado e facilitando o compartilhamento dos dados entre diferentes sistemas. Nesta seção, veremos como exportar dados em formato JSON compactado utilizando Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78176006-7eb7-4af1-96af-6832d0e343af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"compression\", \"gzip\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"{path}/V_OCORRENCIA_AMPLA_JSON.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1784362d-4f42-4536-aeca-e8489a3c6e04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# O CSV tem 20MB e o ZIP ficou com 3MB\n",
    "def get_folder_size(path):\n",
    "    files = dbutils.fs.ls(path)\n",
    "    return sum(f.size for f in files if not f.isDir())\n",
    "\n",
    "folder_size = get_folder_size(f\"{path}/V_OCORRENCIA_AMPLA.json\")\n",
    "print(f\"Tamanho total da pasta CSV: {folder_size} bytes\")\n",
    "\n",
    "folder_size = get_folder_size(f\"{path}/V_OCORRENCIA_AMPLA_JSON.zip\")\n",
    "print(f\"Tamanho total da pasta ZIP: {folder_size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53573172-2c91-400e-920e-9dd34a3b4d11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Lendo Json Compactado\n",
    "\n",
    "A leitura de arquivos JSON compactados é uma prática essencial para lidar com grandes volumes de dados de forma eficiente, otimizando o armazenamento e a transferência. O Apache Spark permite ler arquivos JSON comprimidos em formatos como ZIP ou GZIP, facilitando o processamento distribuído desses dados sem a necessidade de descompactação manual. Nesta seção, veremos como carregar e explorar arquivos JSON compactados utilizando Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cda6873d-0332-445b-bf7d-05c5a8175a97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfCompresao = spark.read \\\n",
    "        .option(\"compression\", \"gzip\") \\\n",
    "        .json(f\"{path}/V_OCORRENCIA_AMPLA_JSON.zip/\") \n",
    "\n",
    "display(dfCompresao)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "07 - Arquivos JSON",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
