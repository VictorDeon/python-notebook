{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3caa998b-5401-4878-b294-264ad3bf81fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Camada Silver: Limpeza e Transformação\n",
    "\n",
    "Aplicar transformações e desnormalizar os dados na camada Silver. Use particionamento para melhorar o desempenho de leitura e escrita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3daebde-3a42-44b5-b92e-c37806f66510",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importar as bibliotecas necessárias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Iniciar a SparkSession com configurações otimizadas\n",
    "# Define um número fixo de partições para shuffle, melhorando o paralelismo                 \n",
    "# Define o tamanho máximo de partições para evitar muitos arquivos pequenos        \n",
    "# Usa o codec Snappy para compressão rápida, otimizando tempo de leitura e escrita    \n",
    "# Habilita otimizações adaptativas, ajustando o número de partições dinamicamente com base no tamanho dos dados\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Transformação Data Silver\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\")  \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128MB\") \\\n",
    "    .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Definir caminhos de armazenamento no Data Lake\n",
    "# Ler dados na Bronze e Armazenar Silver\n",
    "\n",
    "bronze_path = \"/Volumes/workspace/store/bronze/vendas\"\n",
    "silver_path = \"/Volumes/workspace/store/silver/vendas\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c06a0cea-1671-4aa8-9585-1f675d62c053",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Ler o dados da camada Bronze para transformação na camada Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7a6dd23-e28a-47cf-8218-47efd637c883",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ler dados da camada Bronze\n",
    "df_bronze = spark.read.format(\"parquet\").load(bronze_path)\n",
    "display(df_bronze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02982840-980d-4659-b850-e928c7ddc083",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Limpeza dos dados (Cleaning Data)\n",
    "A limpeza de dados é um processo crucial para garantir a qualidade dos dados. Isso envolve a remoção de dados duplicados ou incorretos, a padronização de formatos e valores de dados e o enriquecimento de dados com informações adicionais. Além disso, é importante verificar e corrigir problemas de qualidade, como erros e inconsistências, para garantir que os dados sejam precisos e confiáveis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc7f5a26-d696-4cac-8e19-29de03955200",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    to_date, col, lower, expr, regexp_replace, split, concat, format_number\n",
    ")\n",
    "\n",
    "# Realizar transformações necessárias, incluindo a manipulação do campo EmailNome IdCampanha\n",
    "df_silver = df_bronze \\\n",
    "    .withColumn(\"Data\", to_date(col(\"Data\"), \"yyyy-MM-dd\")) \\\n",
    "    .withColumn(\"Email\", lower(expr(\"regexp_replace(split(EmailNome, ':')[0], '[()]', '')\"))) \\\n",
    "    .withColumn(\"Nome\", expr(\"split(split(EmailNome, ':')[1], ', ')\")) \\\n",
    "    .withColumn(\"Nome\", expr(\"concat(Nome[1], ' ', Nome[0])\")) \\\n",
    "    .withColumn(\"Cidade\", expr(\"split(Cidade, ',')[0]\")) \\\n",
    "    .withColumn(\"TotalVendas\", format_number(col(\"PrecoUnitario\") * col(\"Unidades\"),2)) \\\n",
    "    .withColumn(\"PrecoUnitario\", format_number(col(\"PrecoUnitario\"), 2)) \\\n",
    "    .withColumn(\"CustoUnitario\", format_number(col(\"CustoUnitario\"), 2)) \\\n",
    "    .drop(\"EmailNome\") \\\n",
    "    .drop(\"IdCampanha\")\n",
    "\n",
    "display(df_silver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cf23d72-1bec-4936-8613-9cd6b1418516",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Gravar transformações Silver\n",
    "\n",
    "Particionamento por ano e mês para otimizar consultas baseadas em data, com recomendação de tamanho de arquivo em formato Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3648e62-adc3-423d-b244-aec7bd3baa2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Particionamento por ano e mês para otimizar consultas baseadas em data, com recomendação de tamanho de arquivo\n",
    "\n",
    "df_silver.withColumn(\"Ano\", year(\"Data\")) \\\n",
    "         .withColumn(\"Mes\", month(\"Data\")) \\\n",
    "         .write.option(\"maxRecordsPerFile\", 50000) \\\n",
    "         .partitionBy(\"Ano\", \"Mes\") \\\n",
    "         .format(\"parquet\") \\\n",
    "         .mode(\"overwrite\") \\\n",
    "         .save(silver_path)\n",
    "\n",
    "#Contagem de registros\n",
    "df_silver.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44a2fe68-d244-4010-a424-893569c8d53f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "**Justificativa para particionamento:**\n",
    "\n",
    "partitionBy(\"Ano\", \"Mes\"): Particionar os dados pelas coluna Ano e Mês ajuda a otimizar a leitura quando queremos filtrar ou consultar dados baseados em periodos específicos. Isso reduz o número de arquivos escaneados em consultas, melhorando a performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fada7e8-ddab-4f97-8f58-247a5347d3fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Limpando a Memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "214ea1fd-fa29-41cd-b06f-8f6a12e94809",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "764618d7-5ece-43fa-bb80-dd73179a2c29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "del df_bronze\n",
    "del df_silver"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "003 Transformaçao Silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
